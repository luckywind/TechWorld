# 自我介绍

面试官您好！非常荣幸参与贵公司大数据开发职位的应聘， 我是程幸福，今年31岁，对大数据技术、数据分析有强烈的兴趣，且对流行的Spark技术框架非常熟悉，有很多源码分析经验，以及实践经验。  目前就职于小米科技，担任大数据研发工程师；本次离职主要原因是公司组织架构和组内业务变动很大，与个人职业规划有较大出入。本科毕业于安庆大学数学系，研究生毕业于武汉大学数学与统计学院概率论与数理统计专业。2016年参加工作，至今已有7年工作经验。接下来我简要提两个工作经历：

1. 2016年到2018年在烽火科技担任大数据研发工程师，负责HBase的存储和查询框架设计，并发表专利。

   核心亮点： 

   l 依据业务特点设计RowKey结构，通过二级索引方案支持多种数据索引

   l 通过预分区避免数据读写热点

   l 通过自定义负载均衡算法，增加对IndexTable Region的分配控制。以相同的StartKey作为IndexTable Region和DataTable Region的关联依据，将IndexTable Region和与其对应的DataTable Region分配到同一个RegionServer，减少跨机器读取开销  

2. 2018年到2020年就职于绿盟科技武汉研发中心，担任大数据平台架构师，团队有7人。负责大数据平台的设计和开发工作。负责模块划分、功能拆解；大数据组件Spark、SparkStreaming、Kafka等的调研与应用

2. 2020年至今在小米OneID项目组，负责小米的设备ID、用户ID以及家庭ID的设计与开发工作，对对OneID有更加深入的理解并形成经验，同时对Spark生态中的Core模块、图计算模块、机器学习模块有更加深入的理解。 利用图计算帮助数据工场统计数据血缘图中每个表的父级数据量

以上是我个人的一个简短介绍





# 面试记录

## 民生科技12.21晚7点

平薪/10%涨幅

driver端内存划分，Executor端内存划分；driver内存使用。 排查方法，代码自动debug的方法

Java调优指令

SpingBoot的优势

>  1227完成在线测评

月base? 总包会平摊到多少月？

五险一公积金缴存比例？

有晋升路线吗？

餐补等福利？



通过一些资料以及咨询了一下我的同事，对咱们公司也有个大概的解了，从我的到的消息来看，对于一个7年的硕士来说，年包35确实低了，一般硕5能拿到40。

在你之前其他面试都被我推了，感觉你比较爽快，我顶着发烧接了这个面试，我们算是有缘的，我一定要拿到这个机会， 我还是希望跟您好好谈一谈，要说我现在的待遇，31是最低包，每年有晋升机会。 我期望年包38，这个涨幅只有15%；

31w的基础20%的涨幅，37w

二面准备：

1. 对民生科技的理解，为什么选择民生科技

​        武汉研发中心这边刚刚成立，现在处于快速扩张期，接下来一定会有很多的事情，我觉得这是一个机会；同时本人换工作是非常谨慎的，在当前这个大环境下， 我也希望下家公司是稳定靠谱的，所以我希望这个时候加入，与公司共同成长。

性格测评挂了        



伴伴：

伴伴大数据组件：hive、spark、flink、clickhouse 等，偏上层一些

看中大数据组件了解的深入度和广度

先是问项目，然后问了一些组件和 java 方面的

问的 flink、数据方面的底层原理、业务场景

Flink 建模





## 立得1223：

spark 与Flink各有什么特点？有什么区别？ spark的持久化机制，kafka

spark调优经验：

要做数据中台，但目前就一个大数据开发，预计半年扩到5-6个人； 非结构化数据。 后序有笔试：sql+大数据框架概念

1226:一面通过，发笔试链接

 问题: springboot与spring有什么优势？

1. 并行度优化
   1. 增加executor个数提升并行能力
   2. 增加分区数
2. Join优化
   1. 采用广播器进行Join代替shuffle Join
   2. 让两个RDD共享分区器避免shuffle Join
3. 缓存优化
   1. 公用RDD进行缓存
   2. checkpoint的使用
4. 算子优化
   1. reduceByKey代替groupByKey
5. 调节数据本地性等待时长

0105二面

HBase二级索引、平台、OneID

> 1. 自定义负载均衡算法，以startRowKey为分组依据，把IndexTable的Region与数据表的Region分配到同一个节点上，会不会导致负载不均衡？不会，索引Region和数据Region是一一对应的，数据表Region的负载均衡我们不干预，只要数据表Region是均衡的，索引表自然就均衡(一一对应且是局部索引)
> 2. 数据写入流程
> 3. 宕机容错





## 腾讯地图

1226: 7-8分

1. 业务理解、表达能力欠缺
2. 简历需要更加丰富

1229号二面：
[删除链表重复元素](https://blog.csdn.net/duan20140614/article/details/125722684)

多线程操作syschronize方法，后台线程； 二叉树遍历、递归/非递归两种版本





## 拼多多

两个升序数组中，找到和最小的k个对儿，[每个对儿分别来自两个数组](https://blog.csdn.net/realmagician/article/details/16902927)。

[查找和最小的K对数字(伪暴力法&优先队列)](https://blog.csdn.net/qq_38737428/article/details/122540276?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122540276-blog-119053057.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122540276-blog-119053057.pc_relevant_3mothn_strategy_recovery&utm_relevant_index=2)



## 容知日新1229

> SparkStreaming+Kafka，接收设备日志，进行异常诊断。团队加运维12人

0103

6:00 三段经历，大厂经历。主要问数据量

0106三面HR经理

> 主要聊离职原因，职业规划



## 美的：

笔试0103: 

1. 星型模型和雪花模型的区别和应用
2. 数仓分层，各层职责
3. 输出什么？

```java
obj是null
  if obj=null
     System.out.print(true)
  else     
     System.out.print(false)
```

2:00 面试： 武汉是物流团队，30人左右，全国有2-300人， 上班时间9:00-6:00 制造业公司。







## 金山云:

0104

1. 催促光谷人工智能研究院，院长

2. 催促民生科技二面

3. 催促小红书安排一面 x

4. 金山云一面

   > 8大 组100多号人， 调度组、离线计算、实时计算组。。  明年搬到光谷天地逐渐超过北京。 主要关注我在绿盟带人的经验，spark源码修改的经验。
   >
   > 蔡想面的早，金山云问了我职级低原因，蔡想:因为组织调整的原因。

0106二面

1. 项目
2. 数据接入流程
3. synchronize在静态方法和普通方法上的区别
4. 双亲委派模型
5. 任务调度系统
6. 持续集成的了解
7. Join的优化
8. Flink的反压机制
8. 十亿数据TopN方式：

## 小红书

0106一面

```sql
现在有一张用户下单明细记录表（包含30天的用户下单数据）
dwd_ord_detail_df
(
uid string comment '用户ID', 
oid string comment '订单ID', 
dtm string comment '下单时间，格式：YYYY-MM-DD HH:mm:ss'
)
求每个用户的最长连续下单天数
要求：
1、考虑多段连续下单的场景
2、过滤掉非连续多天下单用户
3、函数名如果忘记，可以随便写一个代替，最后解释一下即可
4、请直接在题目下方写出实现SQL

with t as (
select uid, date_sub(order_date,rn) as diff
from (
select uid,oid,dtm,row_number()over(partition by uid order by dtm) as rn,
	date(dtm) order_date from dwd_ord_detail_df
))
select uid,diff, count(1)cnt from t group by uid,diff 
having cnt>1 ;


with t as (
select 
uid,order_date, row_number()over(partition by uid order by order_date) as rn
  from 
(select uid,date(dtm)order_date from dwd_ord_detail_df group by  uid,date(dtm))-- 去重
),
a as (
select uid,date_sub(order_date,rn) as diff  from t -- 连续日期的开始日期 
),
b as (select uid,diff , count(1) cnt from a group by uid,diff having cnt>1)
select uid,max(cnt) from b group by uid;
```



kyuubi

