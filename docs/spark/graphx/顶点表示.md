https://www.waitingforcode.com/graphx/vertex-representation-apache-spark-graphx/read





顶点除了支持RDD的一些操作外，还支持一些图特有的操作：
aggregateUsingIndex合并两个数据集中相同id点

innerJoin关联两个数据集的点数据

diff比较两个图

除了点ID和属性，VertexRDD还有两个重要的属性，路由表和bitset

# 路由表

graph使用triplet把点传递给边：

*org.apache.spark.graphx.impl.ReplicatedVertexView#upgrade(vertices: VertexRDD[VD], includeSrc: Boolean, includeDst: Boolean)*

```scala
 val shippedVerts:  RDD[(Int, VertexAttributeBlock[VD])] =
        vertices.shipVertexAttributes(shipSrc, shipDst)
          .setName("ReplicatedVertexView.upgrade(%s, %s) - shippedVerts %s %s (broadcast)".format(
            includeSrc, includeDst, shipSrc, shipDst))
          .partitionBy(edges.partitioner.get)
      val newEdges = edges.withPartitionsRDD(edges.partitionsRDD.zipPartitions(shippedVerts) {
        (ePartIter, shippedVertsIter) => ePartIter.map {
          case (pid, edgePartition) =>
            (pid, edgePartition.updateVertices(shippedVertsIter.flatMap(_._2.iterator)))
        }
      })
```



逻辑在*ShippableVertexPartition*'s *shipVertexAttributes(shipSrc: Boolean, shipDst: Boolean)*:

```scala
  /**
   * Generate a `VertexAttributeBlock` for each edge partition keyed on the edge partition ID. The
   * `VertexAttributeBlock` contains the vertex attributes from the current partition that are
   * referenced in the specified positions in the edge partition.
   */
  def shipVertexAttributes(
      shipSrc: Boolean, shipDst: Boolean): Iterator[(PartitionID, VertexAttributeBlock[VD])] = {
    Iterator.tabulate(routingTable.numEdgePartitions) { pid =>
      val initialSize = if (shipSrc && shipDst) routingTable.partitionSize(pid) else 64
      val vids = new PrimitiveVector[VertexId](initialSize)
      val attrs = new PrimitiveVector[VD](initialSize)
      routingTable.foreachWithinEdgePartition(pid, shipSrc, shipDst) { vid =>
        if (isDefined(vid)) {
          vids += vid
          attrs += this(vid)
        }
      }
      (pid, new VertexAttributeBlock(vids.trim().array, attrs.trim().array))
    }
  }
```

这里首次出现路由表，遍历当前分区的顶点相关的所有边分区，收集属性。最后，构建一个Block传给边分区。

graphX假设大多数情况下，边数量大于点数据量，因此构建triplets只把顶点传给边，而不反过来。路由表是一个本地表，用于每个点分区存储相关的所有边分区。

![img](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/graphx_routing_table.png)

> 路由表示例，顶点A出现在所有边分区中，其他的只出现在一个边分区中

# bitmask

除了路由表，每个点分区还有一个bitmask表，它的角色不是特别明显，但是在处理顶点数据也非常有用

*org.apache.spark.graphx.impl.VertexPartitionBaseOps#withMask(org.apache.spark.graphx.impl.VertexPartitionBaseOps#withMask)*

 implementation depends on the vertex partition type. The bitmask is used in already presented map and filter operations:





```scala
def map[VD2: ClassTag](f: (VertexId, VD) => VD2): Self[VD2] = {
    // Construct a view of the map transformation
    val newValues = new Array[VD2](self.capacity)
    var i = self.mask.nextSetBit(0)
    while (i >= 0) {
      newValues(i) = f(self.index.getValue(i), self.values(i))
      i = self.mask.nextSetBit(i + 1)
    }
    this.withValues(newValues)
  }

  def filter(pred: (VertexId, VD) => Boolean): Self[VD] = {
    //不copy值，保留匹配bitmask的点，这样减少map里的数据传输
    // Allocate the array to store the results into
    val newMask = new BitSet(self.capacity)
    // Iterate over the active bits in the old mask and evaluate the predicate
    var i = self.mask.nextSetBit(0)
    while (i >= 0) {
      if (pred(self.index.getValue(i), self.values(i))) {
        newMask.set(i)
      }
      i = self.mask.nextSetBit(i + 1)
    }
    this.withMask(newMask)
  }
```



bitmask的真正威力在于操作多个图，例如，diff/innerJoin 

```scala
val newMask = self.mask & other.mask
      val newValues = new Array[VD2](self.capacity)
      var i = newMask.nextSetBit(0)
      while (i >= 0) {
        newValues(i) = f(self.index.getValue(i), self.values(i), other.values(i))
        i = newMask.nextSetBit(i + 1)
      }
      this.withValues(newValues).withMask(newMask)
```



总结：

1. 路由表用于高效分发顶点数据到边分区
2. bitmask用于过滤顶点，高效实现顶点的join操作