[Introduction to MLlib Pipeline](https://github.com/JerryLead/blogs/blob/master/BigDataSystems/Spark/ML/Introduction%20to%20MLlib%20Pipeline.md)

> 该博主是JerryLead，写的Spark Internal系列很赞

[基于Spark ML Pipeline构建机器学习应用](http://shiyanjun.cn/archives/1693.html) 画出了Spark ML里的类关系图

[Spark 实践–ML Pipelines& 自动调参](https://www.maomaoxiong.net/2018/12/13/spark-%E5%AE%9E%E8%B7%B5-ml-pipelines/)

[Spark MLib 每周一算法](https://ratlsun.github.io/2018/02/19/LogisticRegression/)

**[spark-ml-source-analysis](https://github.com/endymecy/spark-ml-source-analysis)**

> 作者是腾讯的，曾写过[spark-graphx-source-analysis](https://github.com/endymecy/spark-graphx-source-analysis)和 [spark-config-and-tuning](https://github.com/endymecy/spark-config-and-tuning)

[英伟达spark](https://www.nvidia.cn/ai-data-science/spark-ebook/tutorial-gpu-accelerated-xgboost/)

```scala
//缓存提效
    df.cache
    df.createOrReplaceTempView("house")
    spark.catalog.cacheTable("house")
//切分数据
    val Array(trainingData, testData) = df.randomSplit(Array(0.8, 0.2), 1234)
//合并特征向量
 val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol("rawfeatures")
//特征标准化
    val scaler = new StandardScaler().setInputCol("rawfeatures").setOutputCol("features").setWithStd(true).setWithMean(true)
//模型
val rf = new RandomForestRegressor().setLabelCol("medhvalue").setFeaturesCol("features")
//构造pipline
    val steps =  Array(assembler, scaler, rf)
    val pipeline = new Pipeline().setStages(steps)
    val paramGrid = new ParamGridBuilder()
      .addGrid(rf.maxBins, Array(50, 100, 200))
      .addGrid(rf.maxDepth, Array(2, 5, 10))
      .addGrid(rf.numTrees, Array(5, 10, 20))
      .build()

    val evaluator = new RegressionEvaluator()
      .setLabelCol("medhvalue")
      .setPredictionCol("prediction")
      .setMetricName("rmse")

    val crossvalidator = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)
      .setEstimatorParamMaps(paramGrid).setNumFolds(3)

//模型训练
    val pipelineModel = crossvalidator.fit(trainingData)
//取出最好模型
    val rfm = pipelineModel
      .bestModel.asInstanceOf[PipelineModel]
      .stages(2)
      .asInstanceOf[RandomForestRegressionModel]
//特征重要性
 val featureImportances = rfm.featureImportances

    assembler.getInputCols
      .zip(featureImportances.toArray)
      .sortBy(-_._2)
      .foreach {
        case (feat, imp) =>
          println(s"feature: $feat, importance: $imp")
      }
//参数集
    val bestEstimatorParamMap = pipelineModel
      .getEstimatorParamMaps
      .zip(pipelineModel.avgMetrics)
      .maxBy(_._2)
      ._1
    println(s"Best params:\n$bestEstimatorParamMap")
```

