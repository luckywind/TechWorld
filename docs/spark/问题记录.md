File does not exist. Holder DFSClient_does not have any open files

[hdfs超租约异常](https://www.cnblogs.com/wangxiaowei/p/3317479.html)

# 读取parquet失败

```scala
  val frame: DataFrame = spark.read.parquet(idMappingConfig.DWM_DVC_DEVICE_CHAIN + suffix) //.na.drop()
   val res: DataFrame = frame.select("origin_device_id", "user_id", "start_day", "prod_cat_name", "sn", "del_flg").na.fill("")
        res.rdd.map {
                    case Row(origin_device_id: String,user_id: Long,start_day: Long,prod_cat_name: String,sn: String,del_flg: Int) => {
                        DVC_OT_CHAIN(origin_device_id, user_id, start_day.toString, prod_cat_name, end_day, sn, del_flg)
                    }
                }
```

# UncheckedCompileException

[参考](https://newbedev.com/spark-sql-fails-with-java-lang-noclassdeffounderror-org-codehaus-commons-compiler-uncheckedcompileexception)

# NoSuchMethodError: org.apache.parquet.schema.Types$MessageTypeBuilder.addFields

com.twitter的parquet jar代替了hive的，[排除](https://cdap.atlassian.net/browse/CDAP-7082?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel)

# java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V

Scala/spark/spark-sql的版本不匹配

不指定相应依赖的版本，或者直接删除依赖

# 找不到类

1. 尝试include dependency

![image-20220311171321755](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20220311171321755.png)

# java.lang.IllegalStateException: User did not initialize spark context!

去掉setMaster(local)

# parquet类型不匹配

 java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary

parquet有个字段类型是Long,但是表的字段类型是String，导致查询失败

# AbstractMethodError

通常是spark-core版本不匹配

# org.apache.parquet.io.ParquetDecodingException: Could not read file as the Thrift class is not provided and could not be resolved from the file

添加如下参数

```scala
spark.sql.parquet.writeLegacyFormat=true
spark.shuffle.compress=true
spark.network.timeout=600
spark.executorEnv.JAVA_HOME=/opt/soft/jdk1.8.0
spark.serializer=org.apache.spark.serializer.KryoSerializer
```

spark2.0使用sql生成的parquet文件，是不带_common_metadata和_metadata文件的，在spark2.0中是可以使用sc.thriftParquetFile和类文件读取的，但是spark3.0使用这种方式和read.parquet方式就会报这个错。

[参考](https://www.toutiao.com/article/6694170358016115203/?app=news_article&timestamp=1649424210&use_new_style=1&req_id=202204082123300102112180391026D3F8&group_id=6694170358016115203&share_token=22D4F45D-2D84-464B-B8DA-E03C3ED28B9B)

解决办法：但在我这里无效

a)直接采用Spark SQL来读取，而不是Spark代码来读取Parquet文件。

b)通过Spark读取Parquet文件时定义schema

c)启动spark-shell的时候带上启动参数spark.sql.parquet.binaryAsString=true

> Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.

能否sc.thriftParquetFile读取spark3.0的sql生成的文件呢？仍然一样报错。 但是使用spark.read的方式是可以的！

# Empty group: spark_schema

[参考](https://stackoverflow.com/questions/48034825/why-does-streaming-query-fail-with-invalidschemaexception-a-group-type-can-not)

这是因为spark-sql使用了parquet的1.8.1版本，1.8.2及之后的是没问题的

![image-20220505174558511](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20220505174558511.png)

```xml
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-thrift</artifactId>
                  <version>1.8.6</version>
<!--      <version>1.8.1-mdh1.8.1.4</version>-->
   </dependency>
```

