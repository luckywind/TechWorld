[参考](https://mp.weixin.qq.com/s?__biz=MzIwMjA2MTk4Ng==&mid=2247485146&idx=1&sn=2d9cb26aa356baeaf1ced6054329acd0&chksm=96e52717a192ae013c26520f478d209fca557dc5ed9507b2d8b29123b9498bfc0b1f5e3412c3&scene=21#wechat_redirect)

# Task最优位置(节点)的选择

RDD的五个主要特性里

```scala
  *  - A list of partitions
  *                                         一个分区的集合
  *  - A function for computing each split
  *                                         计算每个分割的函数
  *  - A list of dependencies on other RDDs
  *                                         其他RDDS依赖的集合
  *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
  *                                         可选，一个key-value型的RDDS分区（例如说RDD是哈希分区）
  *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
  *    an HDFS file)
  *      可选，一个计算各分在优先位置列表（一个HDFS文件例如块位置）
```

提供一系列的最佳计算位置。

它是如何实现的？ 

DAGScheduler再提交Stage时，会递归提交父Stage的MissingTasks，这里会获取每个分区对应的task的优先位置：

![image-20210903170500558](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20210903170500558.png)

当然谁能告诉它这个最佳位置呢？例如ShuffledRDD是怎么知道自己每个分区的最佳位置？ 通过源码发现是向MapOutputTrackerMaster获取

![image-20210903171034620](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20210903171034620.png)

MapOutputTrackerMaster其实是按照文件大小来选择最佳位置的：
![image-20210903171214910](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20210903171214910.png)

# Task数据本地化的选择

PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY

1. 什么是NO_PREF？
     当Driver应用程序刚刚启动，Driver分配获得的Executor很可能还没有初始化完毕。所以会有一部分任务的本地化级别被设置为NO_PREF,
   如果是ShuffleRDD，其本地行始终为NO_PREF,对于这两种本地化级别是NO_PREF的情况，在任务分配时会被优先分配到非本地节点执行，
   达到一定的优化效果。

2. PROCESS_LOCAL: 数据在同一个 JVM 中，即同一个 executor 上。这是最佳数据 locality。

3. NODE_LOCAL: 数据在同一个节点上。比如数据在同一个节点的另一个 executor上；或在 HDFS 上，恰好有 block 在同一个节点上。速度比
               PROCESS_LOCAL 稍慢，因为数据需要在不同进程之间传递或从文件中读取

4. NO_PREF: 数据从哪里访问都一样快，不需要位置优先

5. RACK_LOCAL: 数据在同一机架的不同节点上。需要通过网络传输数据及文件 IO，比 NODE_LOCAL 慢

6. ANY: 数据在非同一机架的网络上，速度最慢

   

  在 DAGScheduler 向 TaskScheduler 提交了 taskSet 之后，TaskSchedulerImpl 会为每个 taskSet 创建一个
 TaskSetManager 对象，该对象包含taskSet 所有 tasks，并管理这些 tasks 的执行，其中就包括计算 taskSetManager中的 tasks 都有哪些locality levels，以便在调度和延迟调度 tasks 时发挥作用。

 TaskSetManager的主要接口包括：

- ResourceOffer：根据TaskScheduler所提供的单个Resource资源包括 host，executor和locality的要求返回一个合适的Task，TaskSetManager内部会根据上一个任务的成功提交的时间，自动调整自身的Locality匹配策略，如果上一次成功, 提交任务的时间间隔很长，则降低对Locality的要求（例如从最差要求Process Local降低为最差要求Node Local），反之则提高对Locality的要求。**这一动态调整Locality的策略**为了提高任务在最佳Locality的情况下得到运行的机会，因为Resource资源是在短期内分批提供给TaskSetManager的，动态调整Locality门槛有助于改善整体的Locality分布情况。
-  HandleSuccessfulTask/handleFailedTask/handleTaskGettingResult：用于更新任务的运行状态，TaskSetManager在这些函数中除了更新自身维护的任务状态列表等信息，用于剩余的任务的调度以外，也会进一步调用DAGScheduler的函数接口将结果通知给它。
   此外，TaskSetManager在调度任务时还可能进一步考虑Speculattion的情况，当某个任务的运行时间超过其他任务的运行时间的一个特定
   比例值时，该任务可能被重复调度。目的是为了防止某个运行中的Task由于有些特殊原因（例如所在节点CPU负载过高，IO带宽被占等）运
   行缓慢拖延了整个Stage的完成时间，Speculation同样需要根据集群和作业的实际情况合理配置。

 TaskSetManager负责当前TaskSet中所有的Task的启动，失败后的重试，本地化处理等。

