# 普通聚合函数

```sql
countDistinct
first,last
min,max
sum,avg
方差标准差
var_pop, var_samp
stddev, stddev_samp
收集列值
collect_set,  collect_list
```

```scala
 //使用表达式聚合
  df.groupBy("InvoiceNo").agg(count("Quantity").alias("quan"),
    expr("count(Quantity)")).show()
  // 把多个聚合放到一个map里，key是列名，value是聚合函数
  df.groupBy("InvoiceNo").agg("Quantity"->"avg","Quantity"->"stddev_pop").show()
```

# 窗口函数

```scala
  import org.apache.spark.sql.expressions.Window
  val dfWithDate = df.withColumn("date",to_date(col("InvoiceDate"),"MM/d/yyyy H:mm"))
  dfWithDate.createOrReplaceTempView("dfWithDate")
  //先定义窗口
  val windowSpec = Window
    .partitionBy("CustomerId","date")
    .orderBy(col("Quantity").desc)
    .rowsBetween(Window.unboundedPreceding,Window.currentRow)
  val maxPurchaseQuantity=max(col("Quantity")).over(windowSpec)
  val purchaseDenseRank =dense_rank().over(windowSpec)
  val purchaseRank =rank().over(windowSpec)

  dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")
    .select(
      col("CustomerId"),
      col("date"),
      col("Quantity"),
      purchaseRank.alias("quantityRank"),
      purchaseDenseRank.alias("quantityDenseRank"),
      maxPurchaseQuantity.alias("maxPurchaseQuantity")
    ).show()

```

rollup

```scala
  val dfNoNull=dfWithDate.drop()
  dfNoNull.createOrReplaceTempView("dfNoNull")
  val rolledUpDF=dfNoNull.rollup("Date","Country").agg(sum("Quantity"))
    //这里注意：表达式函数外面的反引号
    .selectExpr("Date","Country","`sum(Quantity)`as total_quantity")
    .orderBy("Date")
  rolledUpDF.show()
```

cube

```scala
  dfNoNull.cube("Date","Country").agg(sum(col("Quantity")))
    .select("Date","Country","sum(Quantity)").orderBy("Date").show()
```

