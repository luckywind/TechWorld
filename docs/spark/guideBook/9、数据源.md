# 数据源

spark支持6大核心数据源以及社区开发的上百个数据源：

- csv
- json
- parquet
- orc
- Jdbc/odbc
- text

## 数据源API

### 读

读取数据的核心结构如下

```scala
DataFrameReader.format(...).option("key","value").schema(...).load()
```

这个结构可以读取所有数据源，format是可选的，因为默认按照parquet读取。schema也是可选的，因为数据源可能提供了schema。

DataFrameReader是读取数据的基础类，可以通过sparkSession.read获得。

读取模式

| 读取模式         | 描述                                                        |
| ---------------- | ----------------------------------------------------------- |
| permissive(默认) | 缺损记录作为一个字符串放到_corrupt_record列，其余列置为null |
| dropMalformed    | 丢弃格式错误的记录                                          |
| failFast         | 遇到格式错误，立即失败                                      |

### 写

写数据的核心代码结构如下

```scala
DataFrameWriter.format(...).option("key","value").partitionBy(...).bucketBy(...).sortBy(...)
.save()
```

同样format可选，默认parquet，xxxBy只对文件数据源起作用。

DataFrameWriter是写数据的基础类，可以通过sparkSession.write获取

写入模式

| 写入模式      | 描述             |
| ------------- | ---------------- |
| append        | 追加             |
| overwrite     | 覆盖             |
| errorIfExists | 存在则报错       |
| ignore        | 存在则do nothing |

## Csv文件

### csv选项

| 读/写 | key                      | 接受                                                  | 默认                        | 描述                         |
| ----- | ------------------------ | ----------------------------------------------------- | --------------------------- | ---------------------------- |
| both  | sep                      |                                                       | ,                           | 字段分隔符                   |
| both  | header                   | True/false                                            | false                       | 首行是否是表头               |
| read  | escape                   |                                                       | \                           | 转译符                       |
| read  | inferSchema              | True/false                                            | false                       | 推断类型                     |
| read  | ignoreLeadingWhiteSpace  | true/false                                            | false                       | 跳过前导空白                 |
| read  | ignoreTrailingWhiteSpace | true/false                                            | false                       | 跳过后缀空白                 |
| both  | nullValue                |                                                       | ""                          | 文件中null值的表示           |
| both  | nanValue                 |                                                       | NaN                         | csv文件中NaN或者缺失值的表示 |
| both  | positiveInf              |                                                       | Inf                         | 正无穷表示                   |
| both  | negativeInf              |                                                       | Inf                         | 负无穷表示                   |
| both  | Compression or codec     | None,uncompressed,<br />bzip2,deflategzip,lz4, snappy | none                        | 使用什么压缩编码             |
| both  | dateFormat               |                                                       | Yyyy-MM-dd                  | 日期字段格式                 |
| both  | timestampFormat          |                                                       | yyyy-MM-dd'T'HH:mm:ss.SSSZZ | 时间戳格式                   |
| read  | escapeQuotes             | true/false                                            | true                        | 是否转译引号                 |
| read  | multiLine                | true/false                                            | false                       | 支持跨行记录                 |

### 读csv

```scala
spark.read.format("csv")
  .option("header", "true")
  .option("mode", "FAILFAST")
  .option("inferSchema", "true")
  .load("some/path/to/file.csv")


// COMMAND ----------

// in Scala
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
val myManualSchema = new StructType(Array(
  new StructField("DEST_COUNTRY_NAME", StringType, true),
  new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
  new StructField("count", LongType, false)
))
spark.read.format("csv")
  .option("header", "true")
  .option("mode", "FAILFAST")
  .schema(myManualSchema)
  .load("/data/flight-data/csv/2010-summary.csv")
  .show(5)

```

### 写csv

```scala
csvFile.write.format("csv").mode("overwrite").option("sep", "\t")
  .save("/tmp/my-tsv-file.tsv")
```

## json文件

读写json

```scala
spark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema)
  .load("/data/flight-data/json/2010-summary.json").show(5)

csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")

```

## parquet文件

parquet提供很多存储上的优化，列压缩节省空间，允许读取一列。是spark默认的文件格式。另外它支持复杂类型

parquet读写

```scala
spark.read.format("parquet")
  .load("/data/flight-data/parquet/2010-summary.parquet").show(5)

csvFile.write.format("parquet").mode("overwrite")
  .save("/tmp/my-parquet-file.parquet")
```

## 数据库

读写数据库

```scala
//读sqlite
val driver =  "org.sqlite.JDBC"
val path = "/data/flight-data/jdbc/my-sqlite.db"
val url = s"jdbc:sqlite:/${path}"
val tablename = "flight_info"
import java.sql.DriverManager
val connection = DriverManager.getConnection(url)
connection.isClosed()
connection.close()
//一旦建立了连接，后续就可以读取数据库了
val dbDataFrame = spark.read.format("jdbc").option("url", url)
  .option("dbtable", tablename).option("driver",  driver).load()


//读postgreSql
val pgDF = spark.read
  .format("jdbc")
  .option("driver", "org.postgresql.Driver")
  .option("url", "jdbc:postgresql://database_server")
  .option("dbtable", "schema.tablename")
  .option("user", "username").option("password","my-secret-password").load()
```

## 文本数据

读写文本数据

```scala
spark.read.textFile("/data/flight-data/csv/2010-summary.csv")
  .selectExpr("split(value, ',') as rows").show()

csvFile.select("DEST_COUNTRY_NAME").write.text("/tmp/simple-text-file.txt")
```

