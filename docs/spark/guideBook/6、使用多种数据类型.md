



# 类型使用

第五章讨论了基本的DataFrame概念和抽象，本文会覆盖表达式的构建，回顾不同类型数据的使用

1. Boolean
2. Numbers
3. Strings
4. Dates 和timestamps
5. null值处理
6. 复杂类型
7. UDF

## 转成spark类型

```scala
val df = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("data/retail-data/by-day/2010-12-01.csv")
df.printSchema()
df.createOrReplaceTempView("dfTable")
```



lit函数会把类型转成它对应的spark表示。

```scala
import org.apache.spark.sql.functions.lit
scala> df.select(lit(5), lit("five"), lit(5.0)).show(2)
+---+----+---+
|  5|five|5.0|
+---+----+---+
|  5|five|5.0|
|  5|five|5.0|
+---+----+---+
```

## bool使用

布尔语句由四个元素组成and、or、true和false。

```scala

import org.apache.spark.sql.functions.col
df.where(col("InvoiceNo").equalTo(536365))
  .select("InvoiceNo", "Description")
  .show(5, false)
+---------+-----------------------------------+
|InvoiceNo|Description                        |
+---------+-----------------------------------+
|536365   |WHITE HANGING HEART T-LIGHT HOLDER |
|536365   |WHITE METAL LANTERN                |
|536365   |CREAM CUPID HEARTS COAT HANGER     |
|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|
|536365   |RED WOOLLY HOTTIE WHITE HEART.     |
+---------+-----------------------------------+
```

注意，在spark中，如果想要使用相等，应该使用===和=!=，也可以使用not函数和equalTo方法

```scala
import org.apache.spark.sql.functions.col
df.where(col("InvoiceNo") === 536365)
  .select("InvoiceNo", "Description")
  .show(5, false)
```

## 使用Number

下面的例子是直接在numercal的两个列进行乘法运算，当然也可以使用其他算子

```scala
import org.apache.spark.sql.functions.{expr, pow}
val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)
```

## 使用String

initcap函数： 首字母大写

```scala
import org.apache.spark.sql.functions.{initcap}
df.select(initcap(col("Description"))).show(2, false)
```

还有lower/upper/lpad/ltrim/rpad/rtrim/trim等其他函数

## 空值处理

1. 丢掉null
2. 填充一个值

### Coalesce

coalesce函数从几个列里选择第一个非空值，本例子没有空值，因此直接返回了第一列

```scala
df.select(coalesce(col("Description"),col("CustomerID"))).show()
```

### Ifnull/nullif/nvl/nvl2

1. ifnull默认选择第一个值，如果是空则选择第二个值
2. nullif默认选择第二个值，如果两个相等则返回空

## 复杂类型

### structs

struct字段就相当于一个内嵌的数据框

```scala
  import org.apache.spark.sql.functions.struct
  import org.apache.spark.sql.functions.col
  df.selectExpr("(creation_date,score) as arr","*").show(5,false)
  df.selectExpr("struct(creation_date,score) as arr","*").show(5,false)
  df.select(struct("creation_date","score").alias("arr")).show(5,false)
  df.selectExpr("struct(creation_date,score) as arr","*").select("arr.creation_date").show(5,false)
  df.selectExpr("struct(creation_date,score) as arr","*").select("arr.*").show(5,false)
  df.selectExpr("struct(creation_date,score) as arr","*").select(col("arr").getField("creation_date")).show(5,false)
```

### Array

1. split函数产生数组列
2. size函数计算数组列长度
3. Array_contains函数判断数组是否包含指定值

```scala
  df.select(split(col("creation_date")," ").alias("array_col"))
    .select(col("array_col"),size(col("array_col")).alias("len"))
    .selectExpr("array_col","array_col[0]","array_col[1]","len").show((2))

  df.select(split(col("creation_date")," ").alias("array_col"))
    .select(array_contains(col("array_col"),"2008-08-01")).show(2)

```

### explode

![image-20201021190049114](https://gitee.com/luckywind/PigGo/raw/master/image/image-20201021190049114.png)

```scala
  /**
   * explode
   * explode函数把数组列（或者map列）的每个值产生一行，其他列复制
   * 对应sql:
   * select creation_date,array_col,exploded from (select *, split(creation_date," ")as array_col from df)
   * LATERAL VIEW exploded(array_col) as exploded
   */
  df.withColumn("array_col",split(col("creation_date")," "))
    .withColumn("exploded",explode(col("array_col")))
    .select("creation_date","array_col","exploded").show(false)
```

### map

1. map函数产生map列
2. ['key']获取值

```scala
  df.select(map(col("id"),  col("score")).alias("map"))
    .selectExpr("map['1']").show()
```

### json

1. Get_json_object()获取json的某一个key的值
2. json_tuple获取json数组
3. To_json转成json
4. from_json把json字符串转成struct，需要构造一个jsonSchema

```scala
  import org.apache.spark.sql.functions.{get_json_object,json_tuple,to_json,from_json}
  val jsondf=sparkSession.range(1).selectExpr("""'{"myJSONKey":{"myJSONValue":[1,2,3]}}' as jsonString""")
  jsondf.show(1,false)
  jsondf.select(get_json_object(col("jsonString"),"$.myJSONKey.myJSONValue[1]") as "value",
    json_tuple(col("jsonString"),"myJSONKey")).show(2,false)
  df.selectExpr("(id,score) as myStruct")
    .select(to_json(col("myStruct")))
    .show(5,false)

  val jsonSchema = new StructType(Array(
    new StructField("id",IntegerType,true),
    new StructField("score",IntegerType,true)))
  df.selectExpr("(id,score) as myStruct")
    .select(to_json(col("myStruct")).alias("jsoncol"))
    .select(from_json(col("jsoncol"),jsonSchema),col("jsoncol")).show(2)
```

## udf

udf非常强大，可以使用任何编程语言写，spark在driver上序列化函数代码，发送到所有executor进程。 当用python写的时候，spark会在executor所在机器上启动一个python进程，python进程会跟executor竞争资源，资源不足情况下可能导致节点失败。

**如果注册udf为一个SparkSQL 函数，这样这个函数就可以在表达式中使用，且可以跨语言使用**

```scala
  val numdf: DataFrame = sparkSession.range(5).toDF("num")
  def power3(number:Double):Double =number*number*number
  println(power3(2.0))
  val power3udf=udf(power3(_:Double):Double)
  numdf.select(power3udf(col("num"))).show()

  // 注册udf为一个SparkSQL 函数，这样这个函数就可以在表达式中使用，且可以跨语言使用
  sparkSession.udf.register("power3",power3(_:Double):Double)
  numdf.selectExpr("power3(num)").show()
```



