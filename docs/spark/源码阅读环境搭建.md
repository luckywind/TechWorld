# [参考源码阅读环境](https://linbojin.github.io/2016/01/09/Reading-Spark-Souce-Code-in-IntelliJ-IDEA/)

```shell
（一）
git clone https://github.com/luckywind/spark
cd spark
git remote add apache https://github.com/apache/spark.git
# check remote accounts
git remote -v

（二）
如果apache有新的提交，从apache同步代码过来
 git fetch apache
# Update codes
git pull apache master

（三）
提交修改到自己的仓库
git push origin master
（四）自己新建一个分支，用于开发
git checkout -b dev
git push -u origin dev
git checkout dev

（五）从apache更新我们的dev分支
git checkout master
git pull apache master
git checkout dev
git merge master

```

位置：/Users/chengxingfu/code/open/spark/souceCode/spark

## 构建

1. idea打开项目，导航到pom.xml文件，修改java版本

   ```xml
   # pom.xml
   <java.version>1.8</java.version>
   ```

   2. [参考官方构建文档](https://spark.apache.org/docs/latest/building-spark.html#building-with-sbt),使用下面这个命令编译成功

   ```shell
   ./build/mvn -Pyarn -Dhadoop.version=3.3.0 -DskipTests clean package
   ```

   spark/assembly/target/scala-2.12/jars目录下产出jar包，  加pl参数可以单独构建某一个模块，例如-pl core

   3. 确认已经构建成功
   
      ```shell
      ./bin/spark-shell
      ```
   



## 运行测试

单独构建某个模块

```shell
build/mvn package -DskipTests -pl core
```

运行某个测试

```shell
build/mvn -Dtest=none -DwildcardSuites=org.apache.spark.scheduler.DAGSchedulerSuite test
```

[参考](https://github.com/linbojin/spark-notes)

## codestyle

spark代码使用[scalastyle](http://www.scalastyle.org/)，我们需要配置一个maven插件

```xml
      <plugin>
        <groupId>org.scalastyle</groupId>
        <artifactId>scalastyle-maven-plugin</artifactId>
        <version>1.0.0</version>
        <configuration>
          <verbose>false</verbose>
          <failOnViolation>true</failOnViolation>
          <includeTestSourceDirectory>false</includeTestSourceDirectory>
          <failOnWarning>false</failOnWarning>
          <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
          <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
          <--!  注意配置文件的位置-->
          <configLocation>scalastyle-config.xml</configLocation>
          <outputFile>${basedir}/target/scalastyle-output.xml</outputFile>
          <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
          <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
        </configuration>
        <executions>
          <execution>
            <goals>
              <goal>check</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
```

格式化代码快捷键： ctrl+alt+L



关掉检查

```scala
  // scalastyle:off
  ...  // stuff that breaks the styles
  // scalastyle:on
```

关掉一个rule, by specifying its rule id, as specified in:
  http://www.scalastyle.org/rules-0.7.0.html









## 构建这里遇到问题：

 Server access Error: Connection timed out url=https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.8/ivys/ivy.xml

[改http协议](https://www.cnblogs.com/hwencc/p/5300845.html)

[改国内源](https://blog.csdn.net/zhaorongsheng/article/details/53934542)

最后我把~/.sbt/repositories里typesafe的协议由http改为https就可以了，所有内容如下

```shell
[repositories]
local
osc: http://maven.aliyun.com/nexus/content/groups/public/
typesafe: https://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly
sonatype-oss-releases
maven-central
sonatype-oss-snapshots
```

![image-20200924150551500](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200924150551500.png)

# 核心类

```java
SparkContext.scala 
DAGScheduler.scala
TaskSchedulerImpl.scala
BlockManager.scala
```

[阅读思路](https://blog.csdn.net/rlnLo2pNEfx9c/article/details/107117900)

# sbt

[用户指南](https://www.scala-sbt.org/1.x/docs/zh-cn/Hello.html)

# 基于源码做单元测试

[单元测试](https://z.itpub.net/article/detail/B84888C9E52C8C3D92AB3463F98D7982)

这里使用maven编译时遇到问题

```shell
mvn clean package  -Phive -Phive-thriftserver -Pyarn -DskipTests 
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) on project spark-parent_2.12: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) on project spark-parent_2.12: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed.

```

[参考这里](https://www.cnblogs.com/felixzh/p/10418252.html)把pom中enforce插件里maven版本和java版本和本地一样。Spark Project Parent POM 编译通过

重试又遇到这个错误：

```shell
Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.3.0:compile
```

谷歌发现很多人也遇到了，解决办法就是把Java版本从10改到8，但我这里本来就是8 。

```shell
rm -R /Users/chengxingfu/.m2/repository/org/scala-lang/scala-reflect/*
```

这才编译成功

![image-20200924160312015](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200924160312015.png)

运行一个测试用例[SPARK-3353] 看一下：

![image-20200924160555433](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200924160555433.png)

好了，至此开启spark的源码debug之路吧！

# 运行自带案例

[参考](https://github.com/linbojin/spark-notes/blob/master/ide-setup.md)

## 错误解决

出现错误

```java
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.NoClassDefFoundError: scala/collection/immutable/List
at java.lang.Class.getDeclaredMethods0(Native Method)
at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
at java.lang.Class.privateGetMethodRecursive(Class.java:3048)
at java.lang.Class.getMethod0(Class.java:3018)
at java.lang.Class.getMethod(Class.java:1784)
at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
Caused by: java.lang.ClassNotFoundException: scala.collection.immutable.List
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 7 more
```

打上这个勾include dependencies with Provided scope 就解决了

![image-20200925151101010](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200925151101010.png)

![image-20200925151208400](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200925151208400.png)

打上断点就可以debug了

![image-20200925151712408](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20200925151712408.png)



## object SqlBaseParser is not a member of package org.apache.spark.sql.catalyst.parser

重新编译spark-catalyst

# 编写测试用例

Spark为了确保代码风格一致规范，在项目引入了scala-style checker（比如每个代码文件头部需要定义Apache的License注释；import的顺序等），如果代码不合规范，执行编译会出错。

我们可以模仿Spark源码提供的TestCase来写我们自己的TestCase，可以避免踩到代码风格检查的坑。

下面简单写一个XiaoluobuSuite.scala，主要用来从源码中 获取一个 sql语句对应的 AstTree，以及unresolved logical plan

# 优秀博文

[spark源码解析大全](https://www.cnblogs.com/huanghanyu/p/12989067.html)

# 让Spark程序等待

```scala
Thread.sleep(1000000)
```

# 
