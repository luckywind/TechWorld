[参考](https://www.daimajiaoliu.com/daima/60f902e4549c807)

# org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow

kryo序列化缓存空间不足。

**解决方法：\**增加参数，\**--conf spark.kryoserializer.buffer.max=2047m。**

# **ERROR shuffle.RetryingBlockFetcher: Failed to fetch block shuffle_7_18444_7412, and will not retry**

**原因：**Executor被kill，无法拉取该block。可能是开启AE特性时数据倾斜造成的，其他executor都已完成工作被回收，只有倾斜的executor还在工作，拉取被回收的executor上的数据时可能会拉不到。

**解决方法：\**如果确实是发生了数据倾斜，可以根据该链接的方法进行处理：http://www.jasongj.com/spark/skew/，也可以根据业务逻辑对关键字段\*\*加上distribute by语句\**进行哈希分发来缓解；如果是Spark3以上，或者公司平台的spark2.x源码中定制合入了社区的AE特性，也可以加上这两个参数自动缓解：\**set spark.sql.adaptive.join.enabled=true\**和**set spark.sql.adaptive.enabled=true**。

**可以设置 spark.maxRemoteBlockSizeFetchToMem 参数，设置这个参数以后，超过一定的阈值，会自动将数据 Spill 到磁盘**

# Fail to send RPC to ...

**原因：**数据量过大超出默认参数设置的内存分配阈值，container会被yarn杀掉，其他节点再从该节点拉数据时，会连不上。

**解决方法：**可以优化代码将过多的连续join语句（超过5个）拆分，每3个左右的连续join语句结果生成一个临时表，该临时表再和后面两三个连续join组合，再生成下一个临时表，并在临时表中提前过滤不必要的数据量，使多余数据不参与后续计算处理。**只有在代码逻辑性能和参数合理的前提下，最后才能增加--executor-memory、--driver-memory等资源，不能本末倒置**。



# OOM 

**原因：**代码逻辑或任务参数配置不合理、数据倾斜等导致OOM。

**解决方法：**（1）查看代码中**是否有coalesce()等函数**，该函数相比repartition()不会进行shuffle，处理大分区易造成OOM，如果有则可**换成repartition()**，尽量减少coalesce()的使用；或者是否使用了把所有结果聚集到driver的collect()函数，尽量用**load()\**代替。（2）代码中\**是否有超过5个连续join等不合理的代码逻辑，代码解析与相关对象序列化都是在driver端进行，过于冗余复杂不拆分的代码会造成driver OOM**，可参考第10点优化。（3）查看任务提交参数中**--executor-cores与--executor-memory的比例是否至少为1：4**，一个executor上的所有core共用配置的executor内存，如果有类似2core4G等情况存在，在数据量较大的情况下易OOM，至少应是1core4G或者2core8G等。（4）个别executor的OOM也是数据倾斜会出现的现象之一，如果是这种情况可参考第11点解决。（5）**在代码逻辑和参数合理的前提下，最后一步才是增加资源**，例如--executor-memory=8g。

# driver端频繁出现“Full GC”字样或“connection refused”等日志

driver端此时内存压力很大，无力处理与executor的网络心跳连接等其他工作。

不要广播太多数据，调整广播表的大小阈值，**改过代码也合理调参过了，最后才是增加driver端内存**

分区数由Spark提供的参数控制
 如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到`Failed to connect to host`的错误，也就是executor lost的意思。
 有时候即使不会导致JVM crash也会造成长时间的gc。





# **org.apache.shuffle.FetchFailedException: Connect from xxx closed**

## 解决方案

一般遇到这种问题提高executor内存即可,同时增加每个executor的cpu,这样不会减少task并行度

对于sql作业：

***\*加上参数，\*\*set spark.sql.adaptive.join.enabled=true\*\*和\**set spark.sql.adaptive.enabled=true**，并根据业务数据量合理设置**spark.sql.adaptiveBroadcastJoinThreshold**即broadcast join的小表大小阈值。

#  Executor&Task Lost

因为网络或者gc的原因,worker或executor没有接收到executor或task的心跳反馈

**报错提示**

(1) executor lost

```
WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, aa.local): ExecutorLostFailure (executor lost)
```

(2) task lost

```
WARN TaskSetManager: Lost task 69.2 in stage 7.0 (TID 1145, 192.168.47.217): java.io.IOException: 
Connection from 192.168.47.217:55483 closed
```

(3) 各种timeout

```
ERROR TransportChannelHandler: Connection to 192.168.47.212:35409 has been quiet for 120000 ms while there are outstanding requests. 
Assuming connection is dead; please adjust spark.network.timeout if this is wrong
```

## 解决方案

提高 spark.network.timeout 的值，根据情况改成300(5min)或更高。

默认为 120(120s),配置所有网络传输的延时，如果没有主动设置以下参数，默认覆盖其属性

```
spark.core.connection.ack.wait.timeout
spark.akka.timeout
spark.storage.blockManagerSlaveTimeoutMs
spark.shuffle.io.connectionTimeout
spark.rpc.askTimeout or spark.rpc.lookupTimeout
```

# reduce端缓冲大小以避免OOM

map端的task是不断的输出数据的，数据量可能是很大的。但是，其实reduce端的task，**并不是等到map端task将属于自己的那份数据全部写入磁盘文件之后，再去拉取的**。map端写一点数据，reduce端task就会拉取一小部分数据，立即进行后面的聚合、算子函数的应用。每次reduece能够拉取多少数据，就由buffer来决定。因为拉取过来的数据，都是**先放在buffer**中的。然后才用后面的executor分配的堆内存占比（0.2），hashmap，去进行后续的聚合、函数的执行。可能是会出现，默认是48MB，也许大多数时候，reduce端task一边拉取一边计算，不一定一直都会拉满48M的数据。可能大多数时候，拉取个10M数据，就计算掉了。

大多数时候，也许不会出现什么问题。但是有的时候，map端的数据量特别大，然后写出的速度特别快。reduce端所有task，拉取的时候，全部达到自己的缓冲的最大极限值，缓冲，48M，全部填满。

这个时候，再加上你的reduce端执行的聚合函数的代码，可能会创建大量的对象。也许，一下子，内存就撑不住了，就会OOM。reduce端的内存中，就会发生内存溢出的问题。

## 解决方案

spark.reducer.maxSizeInFlight=48  改为spark.reducer.maxSizeInFlight=24 减少reduce端task缓冲的大小。我宁愿多拉取几次

# shuffle file not found。

比如，executor的JVM进程，可能内存不是很够用了。那么此时可能就会执行GC。minor GC or full GC。总之一旦发生了JVM之后，就会导致executor内，所有的工作线程全部停止，比如BlockManager，基于netty的网络通信。

下一个stage的executor，可能是还没有停止掉的，**task想要去上一个stage的task所在的exeuctor，去拉取属于自己的数据，结果由于对方正在gc，就导致拉取了半天没有拉取到。就很可能会报出，shuffle file not found**。但是，可能下一个stage又重新提交了stage或task以后，再执行就没有问题了，因为可能第二次就没有碰到JVM在gc了。

## 解决

**spark.shuffle.io.maxRetries=3**

第一个参数，意思就是说，shuffle文件拉取的时候，如果没有拉取到（拉取失败），最多或重试几次（会重新拉取几次文件），默认是3次。

**spark.shuffle.io.retryWait=5s**

第二个参数，意思就是说，每一次重试拉取文件的时间间隔，默认是5s钟。

# container内存不足被kill

报错：

Job aborted due to stage failure ExecutorLostFailure (executor 2101 exited caused by one of the running tasks) Reason: Container marked as failed: container_1491814332016_46280_01_009179 on host

处理：

1. 增大分区数，使用 set spark.sql.shuffle.partitions=1000(或更大)
2. 调整代码，减少数据读取量

# 

Job aborted due to stage failure: A shuffle map stage with indeterminate output was failed and retried. However, Spark cannot rollback the ShuffleMapStage 6 to re-process the input data, and has to fail this job. Please eliminate the indeterminacy by checkpointing the RDD before repartition and try again.

# 重试

[失败重试机制](https://www.codenong.com/cs106931464/)

# exit code 143

Executor oom了

