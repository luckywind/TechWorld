# spark3有哪些优化？

主要有四个方面的优化：

1. 自适应执行
   1. 自动调整分区数，分区数过小，意味着分区很大，处理这个分区的task可能需要把数据落盘拖慢查询速度；分区数过大，意味着大量小task执行大量小的网络数据拉取，效率低。 <font color=red>spark3会自动把小的map结果合并，用一个reduce task来处理。</font>
   2. <font color=red>动态切换join策略</font>
      自适应执行会根据stage的统计结果，自动调整更优的join策略
   3. 动态优化倾斜的join，spark3检测到倾斜分区，且分为多个分区启用多个task进行join
2. 动态分区裁剪
   1. Spark会首先过滤维表，根据过滤后的结果找到只需要读事实表的分区，这样就能极大的提升性能
3. join 提示
   1. 也是优化join策略的手段
4. 查询编译加速

# 有哪几种join?

一共五种Join策略：

1. **shuffle hash Join**:
   核心逻辑是参与Join的两个表先按照相同的分区算法进行洗牌，这样同一个key的数据在一个分区里，然后分区对之间进行局部join，而局部join是把小表的分区放入map结构  
   缺点就是需要shuffle,  且因为小表的分区需要放入map结构，所以数据倾斜的情况容易导致OOM
   优点是不需要排序

2. **broadcast hash join**
   如果一个表特别小，直接放入内存然后广播到每个Executor，大表的每个分区直接进行一个map side join即可
   优点是不需要shuffle, 不需要排序，速度非常快
   缺点是需要有个表足够小

3. **sort merge join**

   一般用这个join的场景较多，核心逻辑是两个表先按照相同的分区算法进行洗牌，然后分区内都进行排序，分区对儿之间进行局部join的逻辑是按顺序迭代匹配，匹配上就输出，匹配不上就继续迭代小的一侧。

   优点是适用性较广，对表的大小没有要求

   缺点是需要shuffle和排序

4. **笛卡尔积**

5. **Broadcast Nested Loop Join**

​		核心逻辑是，小表广播到每个Executor,然后大表的每个分区的每条记录执行一个循环匹配，一一去匹配小表的所有记录。效率低下，这个策略是最后的join策略。

# 图

​          最近在研究图计算，考虑到Spark的生态，项目中采用了GraphX框架，相比其他框架，它的优势在于既可以将底层数据看成一个完整的图，也可以对边RDD和顶点RDD使用数据并行处理原语。API大致分为三层，存储层定义了点RDD/边RDD以及三元组；操作层由GraphImpl类和GraphOps类实现； 算法层实现了常见的图算法，且大部分算法都基于Pregel算法实现。

​        Pregel核心原理： 是一种基于BSP模型实现的并行图处理系统。整个流程分为多个超步，每个超步内执行一轮消息传递。核心逻辑：

1. 首先所有顶点接收到一个初始消息进行初始化，用户可定义顶点接收到消息后如何处理，也就是如何更新自己的数据，这个逻辑称为点更新逻辑
2. 接着，接收到消息的顶点成为活跃顶点，没有接收到的顶点进入钝化态，框架统计活跃顶点数和迭代次数如果活跃顶点数大于0且迭代次数没有达到终止条件，就继续按照需求发送消息。消息从活跃的顶点发出，可定义向出边发送/入边发送还是都发送还是不发送。发送的消息内容可以基于原点数据以及边数据构造。接收到消息的顶点再次成为活跃顶点，如果一个顶点接收到多个消息，会先对消息进行聚合，然后再按照顶点更新逻辑更新自己。
3. 当达到终止条件后算法停止

​		目前有几个大的应用： 

1. 统一ID的应用。 采用联通分量算法实现，保持ID的统一和稳定不变； 实践过程中遇到的问题就是，发现有些数据合并错误了，也就是说多个实体，我们错误识别成一个，而分配了一个ID，我们设计了拆分逻辑， 其中有一个非常极端的案例，就是该ID下的id数据太庞大，尝试过直接丢弃掉，优化逻辑后让算法重新合并，但是这时算法的迭代次数会远超平时。这是因为这个巨型的图内部不停的在发送消息。目前的解决办法是找到这种巨型图的业务原因，过滤异常数据，且线上不丢弃它。

2. 大数据平台。数据血缘项目有个需求，数据血缘是一个有向图，顶点是HIve/talos/hdfs/doris等等不同种类的数据，边是数据之间的依赖关系；需求是计算每个数据表处在数据血缘中的第几层，上游不同种类的数据分别有多少。

   1. 数据上游层级这个解决起来比较简单， 核心思想就是下游数据层级=其所有上游数据层级的最大值+1，初始化时，所有数据表的上游层级都是0 ，然后上游向下游发送一个消息，该消息就是上游表的层级+1。 数据表把收到的消息取最大就是其上游数据的层级
   2. 需求二是计算上游不同种类数据源数量，这里需要注意去重，也就是说当前数据表的上游表是其所有父级表的所有上游数据再加上父级表构成，但在统计时需要注意去重。我在实现时采用了Set集合保存当前上游表id自动实现去重
   3. 上述两个需求最复杂的就是图中出现环的情况，开始我是设置了一个最大迭代次数强制结束算法。这会导致上游层数不对。发送消息时判断目的顶点是否在自己的上游集合中，如果在则不再发送。这样就解决了环的问题。

   



# OOM如何解决的？

[参考1](https://blog.csdn.net/yhb315279058/article/details/51035631)

[参考2](https://blog.51cto.com/wang/4634620)

Spark中的OOM问题不外乎以下三种情况：

- map执行中内存溢出
- shuffle后内存溢出
- driver内存溢出
  - collect了大数据集合
  - 作业的task数太多，可考虑缩小分区数
  - 通过参数spark.ui.retainedStages(默认1000)/spark.ui.retainedJobs(默认1000)控制.
  - 增加 内存

**Executor端OOM**

Spark内存模型，一个Executor中的内存主要分为Excution内存和Storage内存和other内存

1. Excution内存是执行内存，join、聚合、shuffle等操作都会用到这个内存，默认占比0.2，是OOM高发区
2. Storage内存是存储broadcast\cache\persist数据的地方，默认占比0.6
3. other是程序预留内存，占比较少

spark1.6之后，Excution和Storage可互相借用，且加入了堆外内存，减少频繁的full gc。

1. map过程产生大量对象导致内存溢出

   > 可在产生大量对象的操作之前增大分区数，从而减少每个task的数据量

2. 数据倾斜导致内存溢出

   > 增大分区数

3. coalesce调用导致内存溢出

   > coalesce减少分区的同时也减少了整个流程task个数(因为没有shuffle)，导致单个task内存占用过大。可用repartition代替，因为它是一个shuffle操作，shuffle前的stage task数量没有减少

4. shuffle后单个文件过大内存溢出

   > shuffle后单个文件过大导致OOM，需要传入分区器，或者传入分区数使用默认的HashPartitioner。

5. 内存评估误差：因为有spill过程，数据倾斜理论上不会导致OOM，但是JVM是采样评估内存占用的， 可能会因为误差导致OOM。

6. 单个Block过大： 单个block是需要足够的内存，可能因为数据倾斜导致单个block过大而OOM，可以设置 spark.maxRemoteBlockSizeFetchToMem 参数，设置这个参数以后，超过一定的阈值，会自动将数据 Spill 到磁盘，此时便可以避免因为数据倾斜造成 OOM 的情况



# 参数哪些调优？

1. driver内存
2. rdd压缩
3. 使用KryoSerializer序列化器
4. 增加本地执行等待时间
5. 推测执行







# SparkSQL调优的场景？问题定位？调优方法？效果？更好的方案？

到底有没有做过？有多深的理解？

指定shuffle分区数, 增加repartition提示

