# thriftServerName

我们的平台会根据ApplicationName检测thriftserver服务的状态，我们在升级spark后，发现2.3版本thriftServerName变了，变成了Thrift JDBC/ODBC Server了，我们想让他改回原来的HiveThriftServer2。

第一步，使用参数--name指定为这个参数，发现不生效，深入源码发现源码不允许指定为这个名字，忽略掉了。

第二步，我们把那行代码注释掉，启动服务时就可以指定为原来的名字了

# thriftserver查询池大小可配置

1. 当有过多查询提交时，会导致后面提交的查询全部排队，延迟较大
2. FIFO模式如果前面一个大的查询，导致后面小查询无法提交
3. Fair模式，过多资源调度使查询延迟更严重



读取本地配置文件

# sql进度

thriftserver上一个sql的执行可能会触发多个Job,且这些job的JobGroup是一样的，但是这个groupID是随机生成的，平台无法跟踪提交的sql对应的groupID是什么。  我们的办法就是定制这个GroupId， 办法如下： 在正常的sql后面追加标记和我们自己生成的GroupID，修改spark源码使用我们自己的groupID代替spark自己生成的groupID, 然后通过Rest服务接口就可以计算进度了





# 优化

## Hive metastore Parquet转换

spark.sql.hive.convertMetastoreParquet=true

[参考](https://developer.aliyun.com/article/761701)

Hive SerDe处理parquet文件的hive表性能一般，Paruqet built in的方式读取数据处理数据更加高效，所以Spark SQL在读写Hive metastore parquet格式的表时，会默认使用自己的Parquet SerDe，而不是采用Hive的SerDe进行序列化和反序列化。

t rue的情况可以使用Parquet 的谓词下推优化

问题： 

**对于Paruqet built in的方式，spark源码中做了一个优化，会缓存schama元数据信息，对于不会变化的静态数据，这个缓存极大提升查询速度。但带来的问题就是缓存长时间不刷新，对于变化的数据查询会出问题。**

解决方案：

1.  可以通过修改spark的HiveMetastoreCatalog类修改其永远不缓存元数据信息
2. 手动refresh table

## 动态资源申请

开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout时间申请一次，直到申请到足够的资源。每次申请的资源量是指数增长的，即1,2,4,8等

