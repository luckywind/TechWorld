# SparkStreaming与Kafka集成

Spark Streaming与kafka 有两种集成方式——receiver模式和direct模式，我司使用direct模式，下面介绍direct模式的具体步骤。

1. **引入依赖包**

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>2.3.1</version>
</dependency>
```

2. **创建一个DStream**

```scala
val stream = KafkaUtils.createDirectStream[String, String](
  streamingContext,
  PreferConsistent,
  Subscribe[String, String](topics, kafkaParams)
)
```

- PreferConsistent（LocationStrategy）

  **第二个参数决定着executor和分区的分配关系**。Spark中内置了三种分区分配策略——PreferBrokers、PreferFixed、PreferConsistent。

  1. PreferBrokers，当kafka集群和yarn nodemanager部署在相同机器时，选用PreferBrokers可以避免增加网络传输。（我司的所有项目部署集群，均不会将Kafka集群和Yarn集群部署在相同机器上，因为Yarn集群通常和HDFS集群部署在一起，HDFS本身就会有大量的磁盘IO开销）

  2. PreferConsistent，executor均匀分配kafka partition。（大多数情况下均采用这种策略）

  3. PreferFixed，显示的指定所有kafka partition数据在哪个机器上处理。

- Subscribe（ConsumerStrategy）

  第三个参数决定在driver、executor上如何构造kafka consumer时，订阅哪些topic或partition。Spark内置了三种机制Subscribe、SubscribePattern、Assign。

  1. Subscribe，设定N个topic去订阅。

  2. SubscribePattern，给定一个正则表达式，kafka集群中所有能够匹配上的topic，均会被订阅。

  3. Assign，指定若干个partition去订阅。

3. **转换函数**

DStream的转换函数和RDD提供的转换函数特别的相似，在调用transform方法后会得到TransformedDStream；在调用map方法后会得到MappedDStream；在调用filter方法后会得到FilteredDStream；在调用foreachRDD方法后会得到ForEachDStream（**更多的转换函数可以查阅DStream.scala源码**）。经过这些函数，最终会生成一个DStreamGraph。（Spark Streaming模块中的“血缘关系”）

其中，值得一提的是foreachRDD，他并不是"动作"函数，并没有立即触发将数据输出到外部数据源的动作。

4. **启动流**

```scala
// ssc,即为StreamingContext实例.

ssc.start()
```

在start背后，Spark Streaming的工作原理如下图所示，图中所示的JobScheduler是Spark Streaming的总调度者。JobScheduler主要由JobGenerator和ReceiverTracker构成，其中JobScheduler中启动了一个定时任务，以batch time为周期，周期性的根据DStreamGraph来生成RDD DAG。

<img src="https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/modb_20220816_cfabdae8-1d4b-11ed-89a0-fa163eb4f6be.png" alt="img" style="zoom:50%;" />

# 保存kafka offset

kafkaUtils的createDirectStream()方式，此方法直接从kafka的broker的分区中读取数据，跳过了zookeeper，并且没有receiver，是spark的task直接对接kakfa topic partition，能保证消息恰好一次语意，但是此种方式因为没有经过zk，topic的offset也就没有保存，当job重启后只能从最新的offset开始消费消息，造成重启过程中的消息丢失。

## 解决方案

一般，有两种方式可以先spark streaming 保存offset：spark checkpoint机制和程序中自己实现保存offset逻辑，下面分别介绍。

## checkpoint机制

spark streaming job 可以通过checkpoint 的方式保存job执行断点，<font color=red>断点中有spark streaming context中的全部信息（包括有kakfa每个topic partition的offset）</font>。checkpoint有两种方式，一个是checkpoint 数据和metadata，另一个只checkpoint metadata，一般情况只保存metadata即可，因此这里只介绍checkpoint metadata。

## 手动保存offset到zk

<font color=red>spark streaming 的rdd可以被转换为HasOffsetRanges类型，进而得到所有partition的offset。</font>

```java
package com.xueba207.test;

import kafka.common.TopicAndPartition;
import kafka.message.MessageAndMetadata;
import kafka.serializer.StringDecoder;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.HasOffsetRanges;
import org.apache.spark.streaming.kafka.KafkaCluster;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.apache.spark.streaming.kafka.OffsetRange;
import scala.Predef;
import scala.Tuple2;
import scala.collection.JavaConversions;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.atomic.AtomicReference;

public class KafkaOffsetExample {

    private static KafkaCluster kafkaCluster = null;

    private static HashMap<String, String> kafkaParam = new HashMap<String, String>();

    private static Broadcast<HashMap<String, String>> kafkaParamBroadcast = null;

    private static scala.collection.immutable.Set<String> immutableTopics = null;

    public static void main(String[] args) {

        SparkConf sparkConf = new SparkConf().setAppName("tachyon-test-consumer");

        Set<String> topicSet = new HashSet<String>();
        topicSet.add("test_topic");


        kafkaParam.put("metadata.broker.list", "test:9092");
        kafkaParam.put("group.id", "com.xueba207.test");

        // transform java Map to scala immutable.map
        scala.collection.mutable.Map<String, String> testMap = JavaConversions.mapAsScalaMap(kafkaParam);
        scala.collection.immutable.Map<String, String> scalaKafkaParam =
                testMap.toMap(new Predef.$less$colon$less<Tuple2<String, String>, Tuple2<String, String>>() {
                    public Tuple2<String, String> apply(Tuple2<String, String> v1) {
                        return v1;
                    }
                });

        kafkaCluster = new KafkaCluster(scalaKafkaParam);

        scala.collection.mutable.Set<String> mutableTopics = JavaConversions.asScalaSet(topicSet);
        immutableTopics = mutableTopics.toSet();
        scala.collection.immutable.Set<TopicAndPartition> topicAndPartitionSet2 = kafkaCluster.getPartitions(immutableTopics).right().get();

        // kafka direct stream 初始化时使用的offset数据
        Map<TopicAndPartition, Long> consumerOffsetsLong = new HashMap<TopicAndPartition, Long>();

        // 没有保存offset时（该group首次消费时）, 各个partition offset 默认为0
        if (kafkaCluster.getConsumerOffsets(kafkaParam.get("group.id"), topicAndPartitionSet2).isLeft()) {

            System.out.println(kafkaCluster.getConsumerOffsets(kafkaParam.get("group.id"), topicAndPartitionSet2).left().get());

            Set<TopicAndPartition> topicAndPartitionSet1 = JavaConversions.setAsJavaSet(topicAndPartitionSet2);

            for (TopicAndPartition topicAndPartition : topicAndPartitionSet1) {
                consumerOffsetsLong.put(topicAndPartition, 0L);
            }

        }
        // offset已存在, 使用保存的offset
        else {

            scala.collection.immutable.Map<TopicAndPartition, Object> consumerOffsetsTemp = kafkaCluster.getConsumerOffsets("com.frey.ys.test", topicAndPartitionSet2).right().get();

            Map<TopicAndPartition, Object> consumerOffsets = JavaConversions.mapAsJavaMap(consumerOffsetsTemp);

            Set<TopicAndPartition> topicAndPartitionSet1 = JavaConversions.setAsJavaSet(topicAndPartitionSet2);

            for (TopicAndPartition topicAndPartition : topicAndPartitionSet1) {
                Long offset = (Long)consumerOffsets.get(topicAndPartition);
                consumerOffsetsLong.put(topicAndPartition, offset);
            }

        }

        JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(5000));
        kafkaParamBroadcast = jssc.sparkContext().broadcast(kafkaParam);

        // create direct stream
        JavaInputDStream<String> message = KafkaUtils.createDirectStream(
                jssc,
                String.class,
                String.class,
                StringDecoder.class,
                StringDecoder.class,
                String.class,
                kafkaParam,
                consumerOffsetsLong,
                new Function<MessageAndMetadata<String, String>, String>() {
                    public String call(MessageAndMetadata<String, String> v1) throws Exception {
                        return v1.message();
                    }
                }
        );

        // 得到rdd各个分区对应的offset, 并保存在offsetRanges中
        final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<OffsetRange[]>();
        JavaDStream<String> javaDStream = message.transform(new Function<JavaRDD<String>, JavaRDD<String>>() {
            public JavaRDD<String> call(JavaRDD<String> rdd) throws Exception {
                OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();
                offsetRanges.set(offsets);
                return rdd;
            }
        });

        // output
        javaDStream.foreachRDD(new Function<JavaRDD<String>, Void>() {

            public Void call(JavaRDD<String> v1) throws Exception {
                if (v1.isEmpty()) return null;

                //处理rdd数据，这里保存数据为hdfs的parquet文件
                HiveContext hiveContext = SQLContextSingleton.getHiveContextInstance(v1.context());
                DataFrame df = hiveContext.jsonRDD(v1);
                df.save("/offset/test", "parquet", SaveMode.Append);


                for (OffsetRange o : offsetRanges.get()) {

                    // 封装topic.partition 与 offset对应关系 java Map
                    TopicAndPartition topicAndPartition = new TopicAndPartition(o.topic(), o.partition());
                    Map<TopicAndPartition, Object> topicAndPartitionObjectMap = new HashMap<TopicAndPartition, Object>();
                    topicAndPartitionObjectMap.put(topicAndPartition, o.untilOffset());

                    // 转换java map to scala immutable.map
                    scala.collection.mutable.Map<TopicAndPartition, Object> testMap =
                            JavaConversions.mapAsScalaMap(topicAndPartitionObjectMap);
                    scala.collection.immutable.Map<TopicAndPartition, Object> scalatopicAndPartitionObjectMap =
                            testMap.toMap(new Predef.$less$colon$less<Tuple2<TopicAndPartition, Object>, Tuple2<TopicAndPartition, Object>>() {
                                public Tuple2<TopicAndPartition, Object> apply(Tuple2<TopicAndPartition, Object> v1) {
                                    return v1;
                                }
                            });

                    // 更新offset到kafkaCluster
                    kafkaCluster.setConsumerOffsets(kafkaParamBroadcast.getValue().get("group.id"), scalatopicAndPartitionObjectMap);

//                    System.out.println(
//                            o.topic() + " " + o.partition() + " " + o.fromOffset() + " " + o.untilOffset()
//                    );
                }
                return null;
            }
        });

        jssc.start();
        jssc.awaitTermination();
    }

}

```







# 背压机制

Spark Streaming在1.5版本开始中引入了Back Pressure机制，大体的思路是——根据最新批次process time、本流的batch interval、处理延迟、调度延迟来<font color=red>估算系统处理能力</font>。（详见PIDRateEstimator.scala）Pid控制法是Spark内部提供的唯一的背压算法。

在Spark streaming中，使用背压机制也十分的方便，主要配置如下：

| 参数                                      | 说明                                                         |
| :---------------------------------------- | :----------------------------------------------------------- |
| spark.streaming.backpressure.enabled      | 是否开启背压机制                                             |
| spark.streaming.kafka.maxRatePerPartition | 每个分区每秒最多拉取多少条数据，因此批次拉取数据量为：批次处理时间**maxRatePerPartition*topic*分区数。 |
| spark.streaming.backpressure.initialRate  | 第一个批次拉取多少条数据。                                   |

在刚接触Spark Streaming背压时，仅仅开启了背压机制，但是没有配置maxRatePerPartition、initialRate，第一个batch拉取的数据量即为kafka topic中的所有数据。

# direct和receiver模式的区别

梳理一下direct相对于receiver模式的优势：

- receiver模式下，spark会在每个executor端起一个receiver线程，该线程会将接收的数据写进blockmanager，在运行时再根据blockId来获取数据；在direct模式下，在运行时RDD的每个分区会直接消费kafka partition，<font color=red>相比之下减少了对磁盘的操作，提高了性能。</font>
- **为了避免数据不丢失，receiver模式下，需要有WAL步骤；而direct模式下，不需要通过预写WAL的方式去达到这点，只需要手动维护offset即可。**
- direct模式下，RDD的并行度取决于Kafka topic的分区数，<font color=red>要修改RDD的并行度更加方便。</font>
- <font color=red>receiver模式下，系统自动提交offset，不对外提供提交offset的接口；direct模式，可手动提交offset。</font>
- <font color=green>存在流量积压时，receiver模式会出现OOM（毕竟会一直往blockmanager中存数据）。而direct模式在积压情况下，并没有真正的读取kafka数据，因此并不会出现OOM。</font>

