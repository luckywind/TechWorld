# DpuSum

当前只支持输出DecimalType
![image-20230609172723733](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230609172723733.png)

### DpuAggregateFunction extends DpuExpression

```scala
trait DpuAggregateFunction extends DpuExpression
  with DpuUnevaluable {
  */***
 *so each reduction function's*
 *** `*initialValues*` *is invoked to populate a single row of output.*
 ** */*
 ** val initialValues: Seq[Expression]

  */***
 ** Using the child reference, define the shape of input batches sent to*
 ** the update expressions*
 ***
 *** ***@note* ***this can be thought of as "pre" update: as update consumes its*
 **       output in order*
 *update之前做的事情*
 **/*
 ** val inputProjection: Seq[Expression]
   //  聚合的前半部分
   // *inputProjection[i] 是updateAggregates[i]的输入*

  def updateAggregates: Seq[AggregateOperator]

  def mergeAggregates: Seq[AggregateOperator]

  */***
 ** 
 ** 使用postMerge的输出计算最终聚合结果*
 *** ***@note* **`*evaluateExpression*` *is bound to* `*aggBufferAttributes*`*, so the references used in*
 *** `*evaluateExpression*` *must also be used in* `*aggBufferAttributes*`*.*
 **/*
 ** val evaluateExpression: Expression

  */** Attributes of fields in aggBufferSchema. */*
 ** def aggBufferAttributes: Seq[AttributeReference]

  */***
 ** Attributes of fields in input aggregation buffers (immutable aggregation buffers that are*
 ** merged with mutable aggregation buffers in the merge() function or merge expressions).*
 ** These attributes are created automatically by cloning the* [[*aggBufferAttributes*]]*.*
 **/*
 ** final lazy val *inputAggBufferAttributes*: Seq[AttributeReference] =
    aggBufferAttributes.map(_.newInstance())

  def sql(isDistinct: Boolean): String = {
    val distinct = if (isDistinct) "DISTINCT " else ""
    s"**$**prettyName(**$**distinct**$**{children.map(_.sql).mkString(", ")})"
  }

  */** String representation used in explain plans. */*
 ** def toAggString(isDistinct: Boolean): String = {
    val start = if (isDistinct) "(distinct " else "("
    prettyName + flatArguments.mkString(start, ", ", ")")
  }

}
```

#### DpuSum

```scala
case class DpuSum(child: Expression, resultType: DataType) extends DpuAggregateFunction {

  val extraDecimalOverflowChecks: Boolean = true
  private lazy val updateIsEmpty = AggregateOperator.AGG_MIN

  private lazy val zeroDec = {
    val dt = resultType.asInstanceOf[DecimalType]
    DpuLiteral(Decimal(0, dt.precision, dt.scale), dt)
  }

  override val initialValues: Seq[DpuLiteral] = resultType match {
    case _:DecimalType if extraDecimalOverflowChecks =>
      Seq(zeroDec, DpuLiteral(true, BooleanType))
    case _ =>
      Seq(DpuLiteral(null, resultType))
  }
  // todo: 后续应加入if表达式来处理null值
  override val inputProjection: Seq[Expression] = resultType match {
    case _: DecimalType if extraDecimalOverflowChecks =>
      // Spark tracks null columns through a second column isEmpty for decimal.
      Seq(DpuCast(child, resultType), DpuIsNull(child))
    case _ => Seq(DpuCast(child, resultType))
  }

  override def updateAggregates: Seq[AggregateOperator] = resultType match {
    case _: DecimalType if extraDecimalOverflowChecks =>
      Seq(AggregateOperator.AGG_SUM, updateIsEmpty)
    case _ => Seq(AggregateOperator.AGG_SUM)
  }

  private lazy val mergeIsEmpty = AggregateOperator.AGG_MIN
  override def mergeAggregates: Seq[AggregateOperator] = resultType match {
    case _: DecimalType if extraDecimalOverflowChecks =>
      Seq(AggregateOperator.AGG_SUM, mergeIsEmpty)
    case _ => Seq(AggregateOperator.AGG_SUM)
  }

  private lazy val sum = AttributeReference("sum", resultType)()
  private lazy val isEmpty = AttributeReference("isEmpty", BooleanType)()
  override val evaluateExpression: Expression = sum

  override def aggBufferAttributes: Seq[AttributeReference] = resultType match {
    case _: DecimalType if extraDecimalOverflowChecks =>
      sum :: isEmpty :: Nil
    case _ => sum :: Nil
  }

  override def nullable: Boolean = false

  override def dataType: DataType = resultType

  override protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression =
    legacyWithNewChildren(newChildren)

  override def children: Seq[Expression] = child :: Nil
}

```




### DpuAggregateExpression extends DpuExpression

```
case class DpuAggregateExpression(origAggregateFunction: DpuAggregateFunction,
                                  mode: AggregateMode,
                                  isDistinct: Boolean,
                                  filter: Option[Expression],
                                  resultId: ExprId)
  extends DpuExpression
```



### HashAggregateExec

被卸载为DpuShuffleExchangeExec

```scala
case class HashAggregateExec(
    requiredChildDistributionExpressions: Option[Seq[Expression]],
    // 递归卸载
    groupingExpressions: Seq[NamedExpression], 
    // 递归卸载
    aggregateExpressions: Seq[AggregateExpression],
    // 递归卸载
    aggregateAttributes: Seq[Attribute],
    initialInputBufferOffset: Int,
    // 递归卸载
    resultExpressions: Seq[NamedExpression],
    // 递归卸载
    child: SparkPlan)
```




### DpuAggregateIterator

DpuShuffleExchangeExec的计算逻辑

```scala
class DpuAggregateIterator(
                            cbIter: Iterator[ColumnarBatch],
                            inputAttributes: Seq[Attribute],
                            groupingExpressions: Seq[NamedExpression],
                            aggregateExpressions: Seq[DpuAggregateExpression],
                            aggregateAttributes: Seq[Attribute],
                            resultExpressions: Seq[NamedExpression],
                            modeInfo: AggregateModeInfo,
                            numOutputRows: SQLMetric,
                            spillSize: SQLMetric,
                            aggTime: SQLMetric
                          )
```



```scala
private[this] val *boundExpressions* = setupReferences()
给aggregate绑定input/final/result引用
```




```scala
  override def next(): ColumnarBatch = {

    val metrics = TaskContext.get().taskMetrics()
    spillSize.set(metrics.memoryBytesSpilled - spillSizeBefore)
    val beforeAgg = System.nanoTime()


    val start = System.nanoTime()
    logInfo("prof: start  aggregate and merge: start time: " + start)
    // aggregate and merge all pending inputs
    logInfo("prof: result Expressions is : " + resultExpressions.map(_.name).toArray.mkString("Array(", ", ", ")"))
    logInfo("prof: result Expressions is : " + resultExpressions.map(_.dataType).toArray.mkString("Array(", ", ", ")"))
    logInfo("prof: result Expressions is : " + resultExpressions.toArray.mkString("Array(", ", ", ")"))
    if (cbIter.hasNext) {
    // 聚合输入并放入*aggregatedBatches*队列，***groupbyOperation***
      aggregateInputBatches()
      if (isMock) {
        mockMergeRaceBatches()
      } else {
    // 合并队列中相邻的batch，直到只有一个batch, ***mergeGroupbyResultOperation***
        tryMergeAggregatedBatches()
      }
    }


    val raceBatch = aggregatedBatches.poll()
    val resKeyColumnHandles = raceBatch.resKeyColumnHandles
    val resValColumnHandles = raceBatch.resValColumnHandles

    logInfo("prof: final batch key handle is : " + resKeyColumnHandles.map(_.getHandle).toArray.mkString("Array(", ", ", ")"))
    logInfo("prof: final batch value handle is : " + resValColumnHandles.map(_.getHandle).toArray.mkString("Array(", ", ", ")"))

    // Get all the result columns after merge batch
    val resVectors = resKeyColumnHandles.map {
      resKey => {
//        logInfo("prof: resKey datatype is : " + RaceDataType.initialRaceDataType(resKey).toString + ", Handle is : " + resKey.getHandle)
        new DpuColumnVector(new RaceColumnVector(resKey))
      }
    } ++ resValColumnHandles.map {
      resVal => {
//        logInfo("prof: resValue datatype is : " + RaceDataType.initialRaceDataType(resVal).toString + ", Handle is : " + resVal.getHandle)
        new DpuColumnVector(new RaceColumnVector(resVal))
      }
    }
    // race计算完后，这里构造一个batch 
    val batch = new ColumnarBatch(resVectors.toArray, resVectors.head.getRaceColumnVector.getRowCount.intValue())

    //  Generate a ColumnarBatch corresponding to resultExpressions
    // 对于avg这种聚合，再次执行一个除法生成最终的batch
    val resultBatch = finalProjectBatch(batch)
    aggTime += NANOSECONDS.toMillis(System.nanoTime() - beforeAgg)
    logInfo("prof: hashAggregate output numRows: " + resultBatch.numRows())
    logInfo("prof: end  aggregate and merge,  use time: " + (System.nanoTime() - start))
    resultBatch
  }
  
```

## 算子卸载

spark.sql(""" select sum(ss_ticket_number) from ds1g.store_sales""").collect()
原始计划

```scala
Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#17]
+- HashAggregate(keys=[], functions=[partial_sum(ss_ticket_number#9)], output=[sum#50L])
   +- FileScan parquet ds1g.store_sales[ss_ticket_number#9] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/store_sales], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_ticket_number:int>
   
```



1. HashAggregateExec算子(nodeName:HashAggregate)会被卸载为DpuHashAggregateExec
    1. 表达式Sum被卸载为DpuSum
2. ShuffleExchangeExec算子(nodeName: Exchange)会被卸载为DpuShuffleExchangeExec(nodeName:DpuColumnarExchange)

物理计划

```scala
DpuColumnarExchange dpusinglepartitioning$(), false, [id=#34]
+- !DpuHashAggregate [partial_dpusum(ss_ticket_number#9, LongType)], [sum#49L], [sum#50L]
   +- DpuFileScan parquet ds1g.store_sales[ss_ticket_number#9] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/user/hive/warehouse/ds1g.db/store_sales], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_ticket_number:int>
   
```