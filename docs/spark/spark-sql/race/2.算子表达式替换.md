# spark-race(二)算子、表达式替换

>回答如何替换算子？如何更改执行计划


SQLExecPlugin注入一个ColumnarRule，即我们的ColumnarOverrideRules：

```scala
case class ColumnarOverrideRules() extends ColumnarRule with Logging {
  lazy val overrides: Rule[SparkPlan] = DpuOverrides()
  lazy val overrideTransitions: Rule[SparkPlan] = new DpuTransitionOverrides()

  override def preColumnarTransitions: Rule[SparkPlan] = overrides

  override def postColumnarTransitions: Rule[SparkPlan] = overrideTransitions
}
```

它重写了preColumnarTransitions和postColumnarTransitions两个方法，分别负责算子替换和计划调整，本文只关注DpuOverrides()

## DpuOverrides

DpuOverrideUtil.*tryOverride*会尝试替换SparkPlan，如果不能替换，则保留原样。
核心接口
def applyOverrides*(*plan: SparkPlan, conf: RaceConf*)*: SparkPlan


### 什么是ReplacementRule?


它是SparkPlan的基本信息，可以帮助确定该类型SparkPlan能否执行替换

DpuOverrides维护一个算子class到算子ReplacementRule的map，共有五类ReplacementRule

* commonExecs:  Map[xxx,  ExecRule]
* commonScans:  Map[xxx, ScanRule]
* commonExpressions: Map[xxx,  ExprRule]
* parts:  Map[xxx,  PartRule]
* dataWriteCmds: Map[xxx, DataWritingCommandRule]

![image-20230609171457149](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230609171524873.png)

>Rule继承关系

RaceMeta把子节点分了5类，每种类型都有自己的抽象

```scala
  val childPlans: Seq[SparkPlanMeta[_]]

  val childExprs: Seq[BaseExprMeta[_]]

  val childScans: Seq[ScanMeta[_]]

  val childParts: Seq[PartMeta[_]]

  val childDataWriteCmds: Seq[DataWritingCommandMeta[_]]
```

 之所以要维护子节点，就是要在wrap、tag和算子的convert时能够完成递归：

1. wrap的递归是在构造Meta类时完成的
2. tag的递归是tagForDpu函数完成的，它会遍历所有子节点
3. 算子的convert的递归是convertIfNeeded()函数完成的
4. 表达式的convert的递归是convertToDpu()函数完成的(表达式的convertToDpu是可convert的算子触发的，所以一定是可convert的)







这个ReplacementRule的构造参数有四个：

1. doWrap函数： **用于把plan封装为一个RaceMeta(一般就是直接基于ReplacementRule的参数进行创建)**, 它维护所有子节点树，以及绑定的表达式树
    后续把原生plan传给第一个参数
2. desc： 节点plan的描述
3. checks: Option*[*TypeChecks*[*_*]] :   用于检查输入、输出类型，具体见[spark-race(三)TypeChecks与tag](https://yusur-first.quip.com/wZAaA5lahPl7)*
4. tag: ClassTag*[*INPUT*] ： //TODO*

expr、exec、scan和partition方法根据传入的不同类型的SparkPlan创建不同类型的ReplacementRule，例如创建ExecRule:

```
  def exec[INPUT <: SparkPlan](
    desc: String,
    pluginChecks: ExecChecks,
    doWrap: (
      INPUT,
      RaceConf,
      Option[RaceMeta[_, _, _]],
      DataFromReplacementRule
    ) => SparkPlanMeta[INPUT]
  )(implicit tag: ClassTag[INPUT]): ExecRule[INPUT] = {
    validateDescAndDoWrapNotNull(desc, doWrap)
    new ExecRule[INPUT](doWrap, desc, Some(pluginChecks), tag)
  }
```

具体的：

```
  exec[FilterExec](
      "The backend for most filter statements",
      ExecChecks(
        (TypeSig.commonRaceTypes + TypeSig.NULL).nested(),
        TypeSig.all
      ),
      (filter, conf, p, r) =>
        new SparkPlanMeta[FilterExec](filter, conf, p, r) {
          override def convertToDpu(): DpuExec =
          //表达式必须转Dpu, 子节点尝试转
            DpuFilterExec(childExprs.head.convertToDpu(), childPlans.head.convertIfNeeded())
      }
    ),
```

针对一个plan，在wrap的时候，根据其class从map中得到对应的Rule,   Rule再调用wrap，内部执行该Rule的doWrap函数把plan封装为一个SparkPlanMeta





### 什么是RaceMeta?

它拿到了具体的plan，信息比ReplacementRule更丰富，是物理计划的一个stage的所有元数据信息，用于决定是否替换、不能替换的原因、何时进行替换，是对ReplacementRule的进一步wrap， 就是调用ReplacementRule的doWrap函数，

其顶层接口如下：

```
abstract class RaceMeta[INPUT <: BASE, BASE, OUTPUT <: BASE](
  val wrapped: INPUT,   //  原生plan
  val conf: RaceConf,
  val parent: Option[RaceMeta[_, _, _]],
  rule: DataFromReplacementRule   // 包含Check 的 Rule
)
```

其子接口有
![image-20230609171524873](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230609171524873.png)
**核心方法**：
def convertToDpu*()*: OUTPUT
def convertToCpu*()*: BASE = wrapped
def tagForDpu*()*: Unit
def tagSelfForDpu*()*: Unit

### wrapPlan

**对整个计划树plan，递归进行wrap,  并wrap其绑定的表达式, 得到一个SparkPlanMeta树。**
doWrap函数创建SparkPlanMeta时，其初始化过程会递归对子节点及其上的表达式进行wrap:

```scala
abstract class SparkPlanMeta[INPUT <: SparkPlan](
  plan: INPUT,
  conf: RaceConf,
  parent: Option[RaceMeta[_, _, _]],// 子节点可访问parent
  rule: DataFromReplacementRule
) extends RaceMeta[INPUT, SparkPlan, DpuExec](plan, conf, parent, rule) {

// 递归进行子算子节点的wrap
override val childPlans:  Seq[SparkPlanMeta[SparkPlan]]  =
 plan.children.map(DpuOverrides.wrapPlan(_, conf,  Some(this)))
// 触发该算子上的表达式进行wrap
override val childExprs:  Seq[BaseExprMeta[_]]  =
 plan.expressions.map(DpuOverrides.wrapExpr(_, conf,  Some(this)))
override val childScans: Seq[ScanMeta[_]] = Seq.empty
override val childParts: Seq[PartMeta[_]] = Seq.empty
override val childDataWriteCmds: Seq[DataWritingCommandMeta[_]] = Seq.empty
```

发现算子树的wrap是递归进行wrap的，表达式树是否也是递归进行wrap的呢？看表达式基类

```scala
abstract class BaseExprMeta[INPUT <: Expression](
  expr: INPUT,
  conf: RaceConf,
  parent: Option[RaceMeta[_, _, _]],
  rule: DataFromReplacementRule
) extends RaceMeta[INPUT, Expression, Expression](expr, conf, parent, rule) {

  private val cannotBeAstReasons: mutable.Set[String] = mutable.Set.empty

  override val childPlans: Seq[SparkPlanMeta[_]] = Seq.empty
  // TODO : we should wrap the child expression here instead of using an empty sequence
  // 递归进行wrap
 override val childExprs: Seq[BaseExprMeta[_]] =
    expr.children.map(DpuOverrides.wrapExpr(_, conf, Some(this)))**
  override val childScans: Seq[ScanMeta[_]] = Seq.empty
  override val childParts: Seq[PartMeta[_]] = Seq.empty
  override val childDataWriteCmds: Seq[DataWritingCommandMeta[_]] = Seq.empty
```

发现，表达式也是递归进行wrap的。
至此，完成算子及表达式的wrap， 即封装了用于判断是否可替换Dpu的所有数据,   可以进行tag了。 

### tag

**def tagForDpu*()*: Unit**
**说明**： RaceMeta的公共接口，先递归对子节点、表达式等进行tag,  最后再tagSelf

```scala
  def tagForDpu(): Unit = {
    childScans.foreach(_.tagForDpu())
    childParts.foreach(_.tagForDpu())
    childExprs.foreach(_.tagForDpu())
    childDataWriteCmds.foreach(_.tagForDpu())
    childPlans.foreach(_.tagForDpu())

    initReasons()

    // TODO : Any reason that this node/sparkplan will not work on the DPU due to the config setting goes heere.

    tagSelfForDpu()
  }
  

```

核心是tagSelfForDpu()
**def tagSelfForDpu*()*: Unit**
**说明**：check当前节点是否支持dpu,   目前只有SparkPlanMeta有实现，主要就是对输入、输出类型进行检查

#### SparkPlanMeta. tagSelfForDpu

对当前算子进行tag， 见[spark-race(三)TypeChecks与tag](https://yusur-first.quip.com/wZAaA5lahPl7)


```scala
 override final def tagSelfForDpu(): Unit = {
   // check列表 逐个对当前Meta进行tag： 检查输入、输出类型
    rule.getChecks.foreach(_.tag(this))
      // 检查childExprs
    if (!canExprTreeBeReplaced) {
      willNotWorkOnDpu("not all expressions can be replaced")
    }
      // 检查childScans
    if (!canScansBeReplaced) {
      willNotWorkOnDpu("not all scans can be replaced")
    }
      // 检查childParts
    if (!canPartsBeReplaced) {
      willNotWorkOnDpu("not all partitioning can be replaced")
    }
      // 检查childDataWriteCmds
    if (!canDataWriteCmdsBeReplaced) {
      willNotWorkOnDpu("not all data writing commands can be replaced")
    }

//    checkExistingTags()
    tagPlanForDpu() // 目前为空
  }
  
```




#### 






### convertPlan

**入口**： 
def doConvertPlan*(
  *wrap: SparkPlanMeta*[*SparkPlan*]*,
  conf: RaceConf,
  optimizations: Seq*[*Optimization*]
)*: SparkPlan
**说明**：  对wrap后的plan进行转dpu，并返回替换后的plan

#### 公共逻辑

```scala
  final def convertIfNeeded(): SparkPlan = {
    if (shouldThisBeRemoved) {
      if (childPlans.isEmpty) {
        throw new IllegalStateException("can't remove when plan has no children")
      } else if (childPlans.size > 1) {
        throw new IllegalStateException("can't remove when plan has more than 1 child")
      }
      //只有一个节点，且可删除，则丢弃并递归检查子节点  //TODO： 什么情况下？alias
      childPlans.head.convertIfNeeded()
    } else {
      //可被替换，且表达式全部支持，则替换为Dpu
      if (canThisBeReplaced && !hasExprUnsupported) {
       // 这里会递归convert子节点
        convertToDpu()
      } else {
       // 这里也会递归convert子节点
        convertToCpu()
      }
    }
  }
```

如何递归触发子节点的convert呢？ 就在RaceMeta的convertToDpu()/convertToCpu()里，也就是在这里创建对应的Dpu算子，例如Filter:

```scala
 new SparkPlanMeta[FilterExec](filter, conf, p, r) {
          override def convertToDpu(): DpuExec =
            DpuFilterExec(childExprs.head.convertToDpu(),// 表达式树替换
                           childPlans.head.convertIfNeeded() // 子节点替换
                                 )
      }

// 父类中未覆盖的convertToCpu():由于当前算子无法替换，只能用withNewChildren方法尝试替换子节点
  override def convertToCpu(): SparkPlan = {
    wrapped.withNewChildren( // 子节点替换
         childPlans.map(_.convertIfNeeded())// 递归进行替换
            )
  }      
  
  
  
  
    final def withNewChildren(newChildren: Seq[BaseType]): BaseType = {
    val childrenIndexedSeq = asIndexedSeq(children)
    val newChildrenIndexedSeq = asIndexedSeq(newChildren)
    // 新子节点集合大小和旧子节点集合大小一定相等
    assert(newChildrenIndexedSeq.size == childrenIndexedSeq.size, "Incorrect number of children")
    if (childrenIndexedSeq.isEmpty ||
        childrenFastEquals(newChildrenIndexedSeq, childrenIndexedSeq)) {
      this
    } else {
      CurrentOrigin.withOrigin(origin) {
        val res = withNewChildrenInternal(newChildrenIndexedSeq)
        res.copyTagsFrom(this)
        res
      }
    }
  }
```


这样通过递归的方式把整颗树完成替换。



### getReasonsNotToReplaceEntirePlan

不能替换整个计划树的原因

```scala
  def getReasonsNotToReplaceEntirePlan: Seq[String] = {
    val childReasons = childPlans.flatMap(_.getReasonsNotToReplaceEntirePlan)
    entirePlanExcludedReasons ++ childReasons
  }
  final def entirePlanExcludedReasons: Seq[String] = {
    cannotReplaceAnyOfPlanReasons.getOrElse(mutable.Set.empty).toSeq
  }
```
rapids有一个配置spark.rapids.allowDisableEntirePlan(默认true)允许插件关闭GPU支持(当某些原因不能使用卸载时), 此时如果不能卸载整棵树，则直接返回原树，关闭插件。

```scala
  private def applyOverrides(plan: SparkPlan, conf: RapidsConf): SparkPlan = {
    val wrap = GpuOverrides.wrapAndTagPlan(plan, conf)
    val reasonsToNotReplaceEntirePlan = wrap.getReasonsNotToReplaceEntirePlan
    if (conf.allowDisableEntirePlan && reasonsToNotReplaceEntirePlan.nonEmpty) {
      if (conf.shouldExplain) {
        logWarning("Can't replace any part of this plan due to: " +
            s"${reasonsToNotReplaceEntirePlan.mkString(",")}")
      }
      plan
    } else {
       ... ...
    }
```


cannotReplaceAnyOfPlanReasons只有一个函数可以写：

```scala
  final def entirePlanWillNotWork(reason: String): Unit = {
    cannotReplaceAnyOfPlanReasons.get.add(reason)
  }
```

这个比较厉害，直接整棵树打回CPU，算子和表达式都可以在tag时进行全树打回！

### recursiveCostPreventsRunningOnDpu

递归打回CPU，直到遇到不能convert的算子

```scala
  final def recursiveCostPreventsRunningOnDpu(): Unit = {
    if (canThisBeReplaced && !mustThisBeReplaced) {
      costPreventsRunningOnDpu()
      childDataWriteCmds.foreach(_.recursiveCostPreventsRunningOnDpu())
    }
  }

  final def costPreventsRunningOnDpu(): Unit = {
    cannotRunOnDpuBecauseOfCost = true
    willNotWorkOnDpu("Removed by cost-based optimizer")
    childExprs.foreach(_.recursiveCostPreventsRunningOnDpu())
    childParts.foreach(_.recursiveCostPreventsRunningOnDpu())
    childScans.foreach(_.recursiveCostPreventsRunningOnDpu())
  }
```



### demo

以select cid, id from table1 where id = 1为例说明
![image-20230609171613359](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230609171613359.png)
当对ProjectExec进行wrap时，先获取对应的ReplacementRule(具体为ExecRule)， 然后调用其wrap方法，而wrap又调用了其构造参数中的doWrap方法：

```
  final def wrap(
    op: BASE,
    conf: RaceConf,
    parent: Option[RaceMeta[_, _, _]],
    r: DataFromReplacementRule
  ): WRAP_TYPE = {
    doWrap(op.asInstanceOf[INPUT], conf, parent, r)
  }
```

doWrap就是创建对应的RaceMeta：

```
*(*proj, conf, p, r*) *=> new DpuProjectExecMeta*(*proj, conf, p, r*)*
```

注意，这里创建DpuProjectExecMeta时会先初始化父类SparkPlanMeta，父类的初始化会对子节点以及绑定的所有表达式进行wrap：

```scala
  override val childPlans: Seq[SparkPlanMeta[SparkPlan]] =
    plan.children.map(DpuOverrides.wrapPlan(_, conf, Some(this)))
  override val childExprs: Seq[BaseExprMeta[_]] =
    plan.expressions.map(DpuOverrides.wrapExpr(_, conf, Some(this)))
```

 这就形成递归wrap，最后把整个plan算子树先转为ReplacementRule，再转为SparkPlanMeta。


表达式树如何转化成BaseExprMeta树的？
先递到叶子节点，也就是FileSourceScanExec节点，该节点对应的ReplacementRule的dowrap函数如何wrap表达式：

```scala
plan.expressions.map(DpuOverrides.wrapExpr(_, conf, Some(this)))
```

plan.expressions是把当前算子的所有表达式收集起来并展平（TODO），再对每个表达式进行wrap:

```scala
  def wrapExpr[INPUT <: Expression](
    expr: INPUT,
    conf: RaceConf,
    parent: Option[RaceMeta[_, _, _]]
  ): BaseExprMeta[INPUT] =
    expressions
      .get(expr.getClass) //    获取对应的ReplacementRule
      .map(r => r.wrap(expr, conf, parent, r)// 调用doWrap函数得到ExprMeta
      .asInstanceOf[BaseExprMeta[INPUT]])
      .getOrElse(new RuleNotFoundExprMeta(expr, conf, parent))
```

可以发现,逻辑和算子的wrap是类似的，这里doWrap函数根据表达式的ReplacementRule进行wrap创建ExprMeta时，其初始化又会递归对子表达式进行wrap：

```scala
abstract class BaseExprMeta[INPUT <: Expression](
  expr: INPUT,
  conf: RaceConf,
  parent: Option[RaceMeta[_, _, _]],
  rule: DataFromReplacementRule
) extends RaceMeta[INPUT, Expression, Expression](expr, conf, parent, rule) {

  private val cannotBeAstReasons: mutable.Set[String] = mutable.Set.empty

  override val childPlans: Seq[SparkPlanMeta[_]] = Seq.empty
  // TODO : we should wrap the child expression here instead of using an empty sequence
  override val childExprs: Seq[BaseExprMeta[_]] =
    expr.children.map(DpuOverrides.wrapExpr(_, conf, Some(this)))
    
    
```


核心逻辑：算子可被替换，且没有不支持的字段

```scala
 final def convertIfNeeded(): SparkPlan = {
    if (shouldThisBeRemoved) {
      if (childPlans.isEmpty) {
        throw new IllegalStateException("can't remove when plan has no children")
      } else if (childPlans.size > 1) {
        throw new IllegalStateException("can't remove when plan has more than 1 child")
      }
      childPlans.head.convertIfNeeded()
    } else {
      if (canThisBeReplaced && !hasExprUnsupported) {
        convertToDpu()
      } else {
        convertToCpu()
      }
    }
  }
```


ProjectMeta会把子节点也尝试转化

```scala
class DpuProjectExecMeta(
                          proj: ProjectExec,
                          conf: RaceConf,
                          p: Option[RaceMeta[_, _, _]],
                          r: DataFromReplacementRule
                        ) extends SparkPlanMeta[ProjectExec](proj, conf, p, r)
  with Logging {
  override def convertToDpu(): DpuExec = {
    // Force list to avoid recursive Java serialization of lazy list Seq implementation
    val dpuExprs = childExprs.map(_.convertToDpu().asInstanceOf[NamedExpression]).toList
    val dpuChild = childPlans.head.convertIfNeeded()
    DpuProjectExec(dpuExprs, dpuChild)
  }
}


```


Filter算子转dpu会执行表达式转dpu，且让子节点尝试转dpu

```scala
 new SparkPlanMeta[FilterExec](filter, conf, p, r) {
          override def convertToDpu(): DpuExec =
            DpuFilterExec(childExprs.head.convertToDpu(), childPlans.head.convertIfNeeded())
      }
      
```