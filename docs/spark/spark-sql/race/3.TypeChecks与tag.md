# spark-race(三)TypeChecks与tag

![image-20230609172018942](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230609172018942.png)


## TypeChecks核心接口

**abstract class TypeChecks[RET]**
**该接口主要是标记当前算子、表达式不支持的类型,** 

* <font color=red>对于ExecChecks,第一个构造参数是支持的类型(typeSig)，用它来检查输入、输出</font>
  
    

```
class ExecChecks private(
    check: TypeSig,
    sparkSig: TypeSig,
    val namedChecks: Map[String, InputCheck],
    override val shown: Boolean = true)
    extends TypeChecks[Map[String, SupportLevel]]
```

* <font color=red>**对于表达式，构造参数是contexts: Map*[*ExpressionContext, ContextChecks*]***</font>

```
abstract class ExprChecks extends TypeChecks[Map[ExpressionContext, Map[String, SupportLevel]]]
case class ExprChecksImpl(contexts: Map[ExpressionContext, ContextChecks])
    extends ExprChecks 
```

在tag表达式时，需要找到对应context（meta的String属性）下的check

**key**类型ExpressionContext有五个子类型，都只有一个toString方法：
ProjectExprContext：“project”
GroupByAggExprContext：“aggregation”
ReductionAggExprContext:“reduction”
WindowAggExprContext:“window”
AstExprContext:“AST”
**value**类型是ContextChecks， 它本身也是TypeChecks的子类,用于检查表达式的输入参数个数、类型和输出类型


>其中AstExprContext比较特别，大多数情况下它是一个优化，在确定操作会被卸载后才检查



### **def tag*(*meta: RaceMeta*[*_, _, _*])*: Unit**

**说明**： 类型检查入口，一般从这里调用tagUnsupportedTypes


### **def  tagUnsupportedTypes:Unit**

**说明**： 检查所有fields是否全被sig支持，有默认实现， 如果存在不支持的类型，则会标记Meta **willNotWorkOnDpu**
检查逻辑： 属于TypeSig的initialTypes，且isSupported中能match上。  见[spark-race(四) TypeSig](https://yusur-first.quip.com/eII7ASVIYlqx)

```scala
// fields中所有字段类型全通过sig检查
def tagUnsupportedTypes(
    meta: RaceMeta[_, _, _],
    sig: TypeSig,// **支持的类型**
    fields: Seq[StructField], // 注意，此时字段已经封装成了StructField
    msgFormat: String
  ): Unit = {
    val unsupportedTypes: Map[DataType, Set[String]] = fields
      .filterNot(attr => sig.isSupportedByPlugin(attr.dataType))
      .groupBy(_.dataType)
      .mapValues(_.map(_.name).toSet)

    if (unsupportedTypes.nonEmpty) {
      meta.willNotWorkOnDpu(msgFormat.format(stringifyTypeAttributeMap(unsupportedTypes)))
    }
  }
  
```



## ExecCheck

<font color=red>用于检查一个SparkPlan节点的输入、输出类型</font>

### 构造器：

```
class ExecChecks private (
  check: TypeSig,// **该算子支持的类型集**
  sparkSig: TypeSig, //spark支持的类型 
  val namedChecks: Map[String, InputCheck],
  override val shown: Boolean = true     // 默认参数
) extends TypeChecks[Map[String, SupportLevel]] 
```

伴生对象提供的构造器：

```
  def apply(check: TypeSig, sparkSig: TypeSig): ExecChecks = {
    new ExecChecks(check, sparkSig, Map.empty)
  }

  def apply(check: TypeSig, sparkSig: TypeSig, namedChecks: Map[String, InputCheck]): ExecChecks = {
    new ExecChecks(check, sparkSig, namedChecks)
  }

```

### tag实现

```
override def tag(raceMeta: RaceMeta[_, _, _]): Unit = {
    val meta = raceMeta.asInstanceOf[SparkPlanMeta[_]]

    // expression.toString to capture ids in not-on-DPU tags
    def toStructField(a: Attribute) = StructField(name = a.toString(), dataType = a.dataType)

   // 输出字段类型检查
    tagUnsupportedTypes(
      meta,
      check,
      meta.outputAttributes.map(toStructField),
      "unsupported data types in output: %s"
    )
    // 输入字段类型检查
    tagUnsupportedTypes(
      meta,
      check,
      meta.childPlans.flatMap(_.outputAttributes.map(toStructField)),
      "unsupported data types in input: %s"
    )

    val namedChildExprs = meta.namedChildExprs
    
    val missing = namedChildExprs.keys.filterNot(namedChecks.contains)
    if (missing.nonEmpty) {
      throw new IllegalStateException(
        s"${meta.getClass.getSimpleName} " +
        s"is missing ExecChecks for ${missing.mkString(",")}"
      )
    }
    
    namedChecks.foreach {
      case (fieldName, pc) =>
        val fieldMeta = namedChildExprs(fieldName)
          .flatMap(_.typeMeta.dataType)
          .zipWithIndex
          .map(t => StructField(s"c${t._2}", t._1))
        tagUnsupportedTypes(
          meta,
          pc.raceTypeSig,
          fieldMeta,
          s"unsupported data types in '$fieldName': %s"
        )
    }
  }
```


在看表达式类型检查之前，我们先了解下ContextChecks

## case class ContextChecks extends TypeChecks

<font color=red>校验表达式输入参数个数、类型和输出类型， 所有表达式的tag工作都会交给相应的ContextChecks, 使用者需要定义这个ContextChecks,它的构造参数决定了支持的类型</font>
主构造函数

```
case class ContextChecks(
  outputCheck: TypeSig, // 支持的输出类型
  sparkOutputSig: TypeSig,  //Spark输出支持的类型，主要用于doc
  paramCheck: Seq[ParamCheck] = Seq.empty,// 参数序列校验
  repeatingParamCheck: Option[RepeatingParamCheck] = None // 重复参数
)
```

<font color=red>检查多个参数、一个输出的表达式，不同的表达式环境下，需要有专门的实例</font>
**方法列表：**
def tagAst*(*exprMeta: BaseExprMeta*[*_*])*: Unit
**说明**： tagBase入口
def tag*(*raceMeta: RaceMeta*[*_, _, _*])*: Unit
**说明**: tagBase的入口
def tagBase*(*raceMeta: RaceMeta*[*_, _, _*]*, willNotWork: String => Unit*)*: Unit
**说明**:   <font color=red>核心方法：检查输出参数类型，输入参数类型、个数</font>

1. 使用outputCheck检查表达式的dataType(即输出类型)
2. **child节点个数不少于paramCheck个数**
3. paramCheck数组和对应的child一一对应，进行tagExprParam
4. 如果repeatingParamCheck为空，则paramCheck个数只能和子表达式个数相等

否则，检查repeatingParamCheck参数类型

1. 检查每个子表达式参数的类型



## ExprChecks extends TypeChecks*[*Map*[*ExpressionContext, Map*[*String, SupportLevel*]]]*

**说明**： 支持tagAst的表达式检查， 且
def tagAst*(*exprMeta: BaseExprMeta*[*_*])*: Unit


### Expr的context

<font color=red>一个表达式的环境不是固定的，通常需要父**算子**节点来决定</font>

```
  lazy val context: ExpressionContext = expr match {
    case _: WindowExpression => WindowAggExprContext
    case _: WindowFunction => WindowAggExprContext
    case _: AggregateFunction => ExpressionContext.getAggregateFunctionContext(this)
    case _: AggregateExpression => ExpressionContext.getAggregateFunctionContext(this)
    case _ => ExpressionContext.getRegularOperatorContext(this)
  }

```

1. 窗口表达式/函数都是window; 
2. 聚合表达式/函数

```
  def getAggregateFunctionContext(meta: BaseExprMeta[_]): ExpressionContext = {
  // 找到父算子
    val parent = findParentPlanMeta(meta)
    assert(parent.isDefined, "It is expected that an aggregate function is a child of a SparkPlan")
    parent.get.wrapped match {
    //如果是普通的算子下的窗口函数算子，则是window
      case agg: SparkPlan if ShimLoader.getSparkShims.isWindowFunctionExec(agg) =>
        WindowAggExprContext
    //如果是聚合算子  
      case agg: BaseAggregateExec =>
        if (agg.groupingExpressions.isEmpty) {
        //且没有grouping， 则是reduction  
          ReductionAggExprContext
        } else {
        // 否则是aggregation
          GroupByAggExprContext
        }
      case _ => throw new IllegalStateException(
        s"Found an aggregation function in an unexpected context $parent")
    }
  }
```

1. 其余都是project



### ExprChecksImpl*(*contexts: Map*[*ExpressionContext, ContextChecks*])* extends ExprChecks

<font color=red>其构造参数是一个map， key就是前面提到的5个类的toString， value是对应的ContextChecks(用来校验表达式的输入、输出)。  </font>

如何使用它们呢？ 
我们来看它如何实现tag： tag是入口

#### tag

```scala
override def tag(meta: RaceMeta[_, _, _]): Unit = {
val exprMeta = meta.asInstanceOf[BaseExprMeta[_]]
val context = exprMeta.context
val checks = contexts.get(context)
if (checks.isEmpty) {
meta.willNotWorkOnDpu(s"this is not supported in the $context context")
} else {
**checks.get.tag(meta)**
}
}
```

其实，每个RaceMeta都有一个context(也就是我们定义的5个字符串key)，它是根据当前表达式的类型定义的，如下所示：

```scala
 lazy val context: ExpressionContext = expr match {
    case _: WindowExpression => WindowAggExprContext
    case _: WindowFunction => WindowAggExprContext
    case _: AggregateFunction => ExpressionContext.getAggregateFunctionContext(this)
    case _: AggregateExpression => ExpressionContext.getAggregateFunctionContext(this)
    case _ => ExpressionContext.getRegularOperatorContext(this)
  }

```

tag的逻辑，就是根据表达式的类型，取出对应的ContextChecks（用于检查表达式输入、输出），然后tag。 
那为什么要用contexts这个map来维护ExpressionContext与ContextChecks之间的关系呢？ 直接一对一不行吗？ 
一个表达式在执行的时候可能涉及到不同的ExpressionContext例如group by /reduce/window /project中，但我们可能只支持在group by /reduce中使用，而不支持在project中使用(例如sum)，从而，在不同的阶段通过map获取到不同的ContextChecks。   这些map的构造在object ExprChecks中进行了预定义，需要用到时直接去拿就行。


#### support

```scala
override def support(
dataType: TypeEnum.Value
): Map[ExpressionContext, Map[String, SupportLevel]] = {
contexts.map {
case (expContext: ExpressionContext, check: ContextChecks) =>
(expContext, check.support(dataType))
}
}
```

#### tagAst

```scala
override def tagAst(exprMeta: BaseExprMeta[_]): Unit = {
val checks = contexts.get(AstExprContext)
if (checks.isEmpty) {
exprMeta.willNotWorkInAst(AstExprContext.notSupportedMsg)
} else {
checks.get.tagAst(exprMeta)
}
}
```



## 预定义的ExprChecksImpl

### projectOnly

```scala
  def projectOnly(
      outputCheck: TypeSig,
      sparkOutputSig: TypeSig,
      paramCheck: Seq[ParamCheck] = Seq.empty,
      repeatingParamCheck: Option[RepeatingParamCheck] = None): ExprChecks =
      // 只能在project环境中用,所以这个map只有一个key/val对
    ExprChecksImpl(Map(
      (ProjectExprContext,
          ContextChecks(outputCheck, sparkOutputSig, paramCheck, repeatingParamCheck))))
          
```



## class CastChecks extends ExprChecks

不支持project外的cast

```scala
  override def tag(meta: RapidsMeta[_, _, _]): Unit = {
    val exprMeta = meta.asInstanceOf[BaseExprMeta[_]]
    val context = exprMeta.context
    if (context != ProjectExprContext) {
      meta.willNotWorkOnGpu(s"this is not supported in the $context context")
    } else {
      tagBase(meta, meta.willNotWorkOnGpu)
    }
  }

  //  拿到被wrap的对象，其输入类型from, 输出类型to
   private[this] def tagBase(meta: RapidsMeta[_, _, _], willNotWork: String => Unit): Unit = {
    val cast = meta.wrapped.asInstanceOf[UnaryExpression]
    val from = cast.child.dataType
    val to = cast.dataType
    if (!gpuCanCast(from, to)) {
      willNotWork(s"${meta.wrapped.getClass.getSimpleName} from $from to $to is not supported")
    }
  }

```

gpuCanCast

```scala
  def gpuCanCast(from: DataType, to: DataType): Boolean = {
    val (checks, _) = getChecksAndSigs(from)
    checks.isSupportedByPlugin(to)
  }
```


def getChecksAndSigs*(*from: DataType*)*: *(*TypeSig, TypeSig*) *
说明：基于from类型，获取插件支持转到的TypeSig,和原生Spark支持转到的TypeSig
输入：from类型
输出：(插件支持转到的类型，spark支持转到的类型)


## case class InputCheck*(*raceTypeSig: TypeSig, spark: TypeSig, notes: List*[*String*] *= *List*.*empty)*

**说明**:  

## case class ParamCheck*(*name: String, raceTypeSig: TypeSig, spark: TypeSig*)*

**说明**：通过位置检查单参数
**raceTypeSig**: race支持的类型
**spark**: spark支持的类型，  目前用于doc

## case class RepeatingParamCheck





## class FileFormatChecks private *(*sig: TypeSig, sparkSig: TypeSig*) *extends TypeChecks*[*SupportLevel*]*