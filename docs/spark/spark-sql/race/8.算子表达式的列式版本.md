# 算子的列式版本改造

Spark的算子的接口类是SparkPlan, 其定义了一个supportsColumnar属性，表示该算子是否支持列式计算，即是否是列式版本，默认是false, 即Spark的算子默认是不支持列式计算的。我们定义一个trait来继承SparkPlan并重写supportsColumnar为true，只要算子混入我们的trait，就说明这个算子是列式计算的。 除此之外，这个算子还需要禁用行式计算的入口函数doExecute()，并且重写doExecuteColumnar()函数，而它是列式算子的执行入口。 

我们以Project算子为例，我们这么定义它的列式版本:

```scala
case class DpuProjectExec(projectList: List[NamedExpression], child: SparkPlan) extends UnaryExecNode with DpuExec {
  override def doExecute(): RDD[InternalRow] =
    throw new IllegalStateException(s"Row-based execution should not occur for $this")

  override def doExecuteColumnar(): RDD[ColumnarBatch] = {
    val boundProjectList = DpuBindReferences.bindDpuReferences(projectList, child.output)
    val rdd = child.executeColumnar()
    val res = rdd.map { cb =>
      val project:ColumnarBatch = DpuProjectExec.projectAndClose(cb, boundProjectList)
      project
    }
    res
  }
}
```

在这个定义中，我们重写了doExecuteColumnar方法，它先调用child.executeColumnar()触发上游的计算，上游计算的结果是一个RDD，其数据是列式数据。然后把上游计算的结果作为输入，触发该算子承载的列式表达式的计算。



# 表达式的列式版本改造

SparkSQL中表达式的接口是Expression，计算入口是eval函数，是行式执行的，为了区分表达式的行式计算与列式计算，我们定义一个trait DpuExpression继承Expression，执行入口改成columnarEval(batch: ColumnarBatch)函数，

```scala
trait DpuExpression extends Expression with Arm with Logging{
  final override def eval(input: InternalRow = null): Any =
    throw new UnsupportedOperationException(s"Cannot evaluate expression: $this")
  def columnarEval(batch: ColumnarBatch): Any
}
```

这个定义中我们禁用了eval函数，并且定义了columnarEval函数作为执行入口，可以看到它拿到的数据是ColumnarBatch即列式数据。

而对列式数据的操作，如果DPU支持，则会把数据拷贝到DPU卡上进行计算，如果DPU不支持，则直接在CPU上计算。





DPU卡可以理解为流式计算器，数据边流入边进行计算并流出。