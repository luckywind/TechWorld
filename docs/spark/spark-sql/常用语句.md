# 建表

```sql
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] catalog.dbname.table_name
      [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]
      [COMMENT table_comment]
      [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
      STORED AS parquet
```

示例

```sql
CREATE TABLE IF NOT EXISTS hive_zjyprc_hadoop.dwm.testxx (
        social string COMMENT '社会单元id',
        extend_map map < string,string > COMMENT '设备数量统计',
        date int COMMENT '分区'
      ) COMMENT 'this is a table description' PARTITIONED BY (date) STORED AS parquet;
```



```sql
with tmp as (
    select stack(2,
             'A',10,date '2015-01-01',
             'B',20,date '2016-01-01') 
             as (a,b,c)
)
SELECT * from tmp



with tmp as (
    select stack(2,
             0,1,11,111,
             1,2,22,222) 
             as (id,height,weight,age)
)
SELECT * from tmp;
```



# 其他

### stack的用法

作用：利用一些表达式产生n行数据，默认列名为col0, col1,等，可以指定列名

语法：

```sql
stack(n, expr1, expr2.. exprk) as (a,b,c,...)
```

第一个参数是行数，后续是表达式，执行表达式产生数据

例如：

```sql
+-------+---------+-----+---------+----+
 |   Name|Analytics|   BI|Ingestion|  ML|
 +-------+---------+-----+---------+----+
 | Mickey|     null|12000|     null|8000|
 | Martin|     null| 5000|     null|null|
 |  Jerry|     null| null|     1000|null|
 |  Riley|     null| null|     null|9000|
 | Donald|     1000| null|     null|null|
 |   John|     null| null|     1000|null|
 |Patrick|     null| null|     null|1000|
 |  Emily|     8000| null|     3000|null|
 |   Arya|    10000| null|     2000|null|
 +-------+---------+-----+---------+----+ 
 想删除空值，得到
 +-------+---------+---------------+
 |   Name|  Project|Cost_To_Project|
 +-------+---------+---------------+
 | Mickey|       BI|          12000|
 | Mickey|       ML|           8000|
 | Martin|       BI|           5000|
 |  Jerry|Ingestion|           1000|
 |  Riley|       ML|           9000|
 | Donald|Analytics|           1000|
 |   John|Ingestion|           1000|
 |Patrick|       ML|           1000|
 |  Emily|Analytics|           8000|
 |  Emily|Ingestion|           3000|
 |   Arya|Analytics|          10000|
 |   Arya|Ingestion|           2000|
 +-------+---------+---------------+ 
 可以通过stack语句对每个Name产生四行数据，每行两个列，第一列是硬编码的字符串，第二列从数据中抽取
 pivotDF.select($"Name", expr("stack(4, 'Analytics', Analytics, 'BI', BI, 'Ingestion', Ingestion, 'ML', ML) as (Project, Cost_To_Project)")).show(false)
 +------+---------+---------------+
 |Name  |Project  |Cost_To_Project|
 +------+---------+---------------+
 |Mickey|Analytics|null           |
 |Mickey|BI       |12000          |
 |Mickey|Ingestion|null           |
 |Mickey|ML       |8000           |
 |Martin|Analytics|null           |
 |Martin|BI       |5000           |
 |Martin|Ingestion|null           |
 |Martin|ML       |null           |
 |Jerry |Analytics|null           |
 |Jerry |BI       |null           |
 |Jerry |Ingestion|1000           |
 |Jerry |ML       |null           |
 |Riley |Analytics|null           |
 |Riley |BI       |null           |
 |Riley |Ingestion|null           |
 |Riley |ML       |9000           |
 |Donald|Analytics|1000           |
 ----
 ----
 最终过滤掉null值即可
```



完整代码：

```sql
val data = Seq(
       ("Ingestion", "Jerry", 1000), ("Ingestion", "Arya", 2000), ("Ingestion", "Emily", 3000),
       ("ML", "Riley", 9000), ("ML", "Patrick", 1000), ("ML", "Mickey", 8000),
       ("Analytics", "Donald", 1000), ("Ingestion", "John", 1000), ("Analytics", "Emily", 8000),
       ("Analytics", "Arya", 10000), ("BI", "Mickey", 12000), ("BI", "Martin", 5000))
 import spark.sqlContext.implicits._
 val df = data.toDF("Project", "Name", "Cost_To_Project")
 --pivot
 val pivotDF = df.groupBy("Name").pivot("Project").sum("Cost_To_Project")
 pivotDF.show()
 --unpivot
 val unPivotDF = pivotDF.select($"Name", expr("stack(4, 'Analytics', Analytics, 'BI', BI, 'Ingestion', Ingestion, 'ML', ML) as (Project, Cost_To_Project)")).where("Cost_To_Project is not null")
 unPivotDF.show() 
```





1. 删除

```sql
insert overwrite tmp 
select * from tmp where id != '666';
```

2. 更新

```sql
insert overwrite tmp 
select id,label,
       if(id = '1' and label = 'grade','25',value) as value 
from tmp where id != '666';
```

3. 行转列

```sql
-- Step03：最后将info的内容切分
select id,split(info,':')[0] as label,split(info,':')[1] as value
from 
(
-- Step01：先将数据拼接成“heit:180,weit:60,age:26”
    select id,concat('heit',':',height,',','weit',':',weight,',','age',':',age) as value 
    from tmp
) as tmp
-- Step02：然后在借用explode函数将数据膨胀至多行
lateral view explode(split(value,',')) mytable as info;
```



3. 列转行

```sql
with tmp2 as (
    select stack(5,
             0,'height',11,
             0,'weight',22, 
             0,'age',18 ,
             1,'weight',33,
             2,'age',18   ) 
             as (id,label,value)
)
```



法一：

```sql
-- 每个列都单独查出来，再按照id进行join
select 
tmp1.id as id,tmp1.value as height,tmp2.value as weight,tmp3.value as age 
from 
(select id,label,value from tmp2 where label = 'heit') as tmp1
join
on tmp1.id = tmp2.id
(select id,label,value from tmp2 where label = 'weit') as tmp2
join
on tmp1.id = tmp2.id
(select id,label,value from tmp2 where label = 'age') as tmp3
on tmp1.id = tmp3.id;
```



法二

```sql
with tmp2 as (
    select stack(5,
             0,'height',11,
             0,'weight',22, 
             0,'age',18 ,
             1,'weight',33,
             2,'age',18   ) 
             as (id,label,value)
)

-- step2: 从map里解析出每一个列
select
id,tmpmap['height'] as height,tmpmap['weight'] as weight,tmpmap['age'] as age
from 
(
  -- step1: 把同一个id的值收集到一个map里
    select id,
           str_to_map(concat_ws(',',collect_set(concat(label,':',value))),',',':') as tmpmap  
    from tmp2 group by id
) as tmp1;
```



3. 分析函数

```sql
select id,label,value,
       lead(value,1,0)over(partition by id order by label) as lead,
       lag(value,1,999)over(partition by id order by label) as lag,
       first_value(value)over(partition by id order by label) as first_value,
       last_value(value)over(partition by id order by label) as last_value
from tmp;
```



```sql
select id,label,value,
       row_number()over(partition by id order by value) as row_number,
       rank()over(partition by id order by value) as rank,
       dense_rank()over(partition by id order by value) as dense_rank
from tmp;
```



3. 多维分析





3. 数据倾斜groupby

```sql
with tmp1 as (
    select stack(2,
             0,'height','v1',
             1,'weight','v2') 
             as (id,label,value)
)
-- step3: 再按key进行聚合
select label,sum(cnt) as all from 
(
  -- step2：key和随机数一起先聚合
    select rd,label,sum(1) as cnt from 
    (   -- step1: 先给每一行生成一个随机数
        select id,round(rand(),2) as rd,label,value from tmp1
    ) as tmp
    group by rd,label
) as tmp
group by label;
```



3. 数据倾斜join

```sql
with a as (
    select stack(5,
             0,'height',11,
             0,'weight',22, 
             0,'age',18 ,
             1,'weight',33,
             2,'age',18   ) 
             as (id,label,value)
),
b as (
    select stack(5,
             0,'height',11,
             0,'weight',22, 
             0,'age',18 ,
             1,'weight',33,
             2,'age',18   ) 
             as (id,label,value)
)

-- step2:再按key进行聚合
select label,sum(value) as all from 
(
    select rd,label,sum(value) as value from
    (
      -- step1: 两个表都加一个随机数列，并按照key和随机数一起join， 
        select tmp1.rd as rd,tmp1.label as label,tmp1.value*tmp2.value as value 
        from 
        (
            select id,round(rand(),1) as rd,label,value from a
        ) as tmp1
        join
        (
            select id,rd,label,value from b
            lateral view explode(split('0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9',',')) mytable as rd
        ) as tmp2
        on tmp1.rd = tmp2.rd and tmp1.label = tmp2.label
    ) as tmp1
    group by rd,label
) as tmp1
group by label;
```

