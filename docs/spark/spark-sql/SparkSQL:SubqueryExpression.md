# SparkSQLä¸­çš„subqueries

## where ä¸­çš„ç­‰å¼å€¼

```sql
    val df = spark.sql("select i_item_sk,i_item_id" +
      "  from ds1g.item where i_item_sk=(select i_item_sk from ds1g.item where i_item_sk=1)")
    df.explain(true)
```

```mathematica
== Parsed Logical Plan ==
'Project ['i_item_sk, 'i_item_id]
+- 'Filter ('i_item_sk = scalar-subquery#0 [])
   :  +- 'Project ['i_item_sk]
   :     +- 'Filter ('i_item_sk = 1)
   :        +- 'UnresolvedRelation [ds1g, item], [], false
   +- 'UnresolvedRelation [ds1g, item], [], false

== Analyzed Logical Plan ==
i_item_sk: int, i_item_id: string
Project [i_item_sk#1, i_item_id#2]
+- Filter (i_item_sk#1 = scalar-subquery#0 [])
   :  +- Project [i_item_sk#23]
   :     +- Filter (i_item_sk#23 = 1)
   :        +- SubqueryAlias spark_catalog.ds1g.item
   :           +- Relation ds1g.item[i_item_sk#23,i_item_id#24,i_rec_start_date#25,i_rec_end_date#26,i_item_desc#27,i_current_price#28,i_wholesale_cost#29,i_brand_id#30,i_brand#31,i_class_id#32,i_class#33,i_category_id#34,i_category#35,i_manufact_id#36,i_manufact#37,i_size#38,i_formulation#39,i_color#40,i_units#41,i_container#42,i_manager_id#43,i_product_name#44] parquet
   +- SubqueryAlias spark_catalog.ds1g.item
      +- Relation ds1g.item[i_item_sk#1,i_item_id#2,i_rec_start_date#3,i_rec_end_date#4,i_item_desc#5,i_current_price#6,i_wholesale_cost#7,i_brand_id#8,i_brand#9,i_class_id#10,i_class#11,i_category_id#12,i_category#13,i_manufact_id#14,i_manufact#15,i_size#16,i_formulation#17,i_color#18,i_units#19,i_container#20,i_manager_id#21,i_product_name#22] parquet

== Optimized Logical Plan ==
Project [i_item_sk#1, i_item_id#2]
+- Filter (isnotnull(i_item_sk#1) AND (i_item_sk#1 = scalar-subquery#0 []))
   :  +- Project [i_item_sk#23]
   :     +- Filter (isnotnull(i_item_sk#23) AND (i_item_sk#23 = 1))
   :        +- Relation ds1g.item[i_item_sk#23,i_item_id#24,i_rec_start_date#25,i_rec_end_date#26,i_item_desc#27,i_current_price#28,i_wholesale_cost#29,i_brand_id#30,i_brand#31,i_class_id#32,i_class#33,i_category_id#34,i_category#35,i_manufact_id#36,i_manufact#37,i_size#38,i_formulation#39,i_color#40,i_units#41,i_container#42,i_manager_id#43,i_product_name#44] parquet
   +- Relation ds1g.item[i_item_sk#1,i_item_id#2,i_rec_start_date#3,i_rec_end_date#4,i_item_desc#5,i_current_price#6,i_wholesale_cost#7,i_brand_id#8,i_brand#9,i_class_id#10,i_class#11,i_category_id#12,i_category#13,i_manufact_id#14,i_manufact#15,i_size#16,i_formulation#17,i_color#18,i_units#19,i_container#20,i_manager_id#21,i_product_name#22] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Filter (isnotnull(i_item_sk#1) AND (i_item_sk#1 = Subquery subquery#0, [id=#12]))
   :  +- Subquery subquery#0, [id=#12]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- Filter (isnotnull(i_item_sk#23) AND (i_item_sk#23 = 1))
   :           +- FileScan parquet ds1g.item[i_item_sk#23] Batched: true, DataFilters: [isnotnull(i_item_sk#23), (i_item_sk#23 = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk), EqualTo(i_item_sk,1)], ReadSchema: struct<i_item_sk:int>
   +- FileScan parquet ds1g.item[i_item_sk#1,i_item_id#2] Batched: true, DataFilters: [isnotnull(i_item_sk#1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string>

```



## in è¡¨è¾¾å¼ä¸­çš„

```sql
select i_item_sk,i_item_id from ds1g.item where i_item_sk in (select i_item_sk from ds1g.item where i_item_sk=1)
```

å˜æˆäº†hashJoin

```sql
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- BroadcastHashJoin [i_item_sk#912], [i_item_sk#934], LeftSemi, BuildRight, false
   :- FileScan parquet ds1g.item[i_item_sk#912,i_item_id#913] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i_item_sk:int,i_item_id:string>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#20]
      +- Filter (isnotnull(i_item_sk#934) AND (i_item_sk#934 = 1))
         +- FileScan parquet ds1g.item[i_item_sk#934] Batched: true, DataFilters: [isnotnull(i_item_sk#934), (i_item_sk#934 = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk), EqualTo(i_item_sk,1)], ReadSchema: struct<i_item_sk:int>

```

## projectä¸­çš„

```sql
select  (select max(i_item_sk) from ds1g.item )
```

```sql

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [Subquery subquery#911, [id=#20] AS scalarsubquery()#936]
   :  +- Subquery subquery#911, [id=#20]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- HashAggregate(keys=[], functions=[max(i_item_sk#912)], output=[max(i_item_sk)#935])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#18]
   :              +- HashAggregate(keys=[], functions=[partial_max(i_item_sk#912)], output=[max#939])
   :                 +- FileScan parquet ds1g.item[i_item_sk#912] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i_item_sk:int>
   +- Scan OneRowRelation[]
```





- Table Subquery

  ```sql
  SELECT *
  FROM   (SELECT *
          FROM   t1) AS tab;    -- åªå­˜åœ¨é€»è¾‘è®¡åˆ’ä¸­
  ```

- Scalar Subquery Expressions

  ```sql
  SELECT col2,
         (SELECT Max(col1)
          FROM   t1) AS col1     -- ScalarSubqueryè¡¨è¾¾å¼
  ```

  

- Subquery in WHERE clause

  ```sql
  SELECT *
  FROM   t1
  WHERE  col1 IN (SELECT col1
                  FROM   t2);      -- ListQuery è½¬ä¸ºjoin
  ```

  

- Correlated Subquery
  å­æŸ¥è¯¢ä½¿ç”¨å¤–æŸ¥è¯¢çš„åˆ—ï¼Œ åªæ”¯æŒç­‰å€¼join

  ```sql
  SELECT *
  FROM   t1 AS t1
  WHERE  EXISTS (SELECT 1
                 FROM   t2 AS t2
                 WHERE  t1.col1 = t2.col1); -- join
  ```

  

- Correlated Scalar Subquery
  å­æŸ¥è¯¢ä½¿ç”¨äº†å¤–æŸ¥è¯¢çš„åˆ—ï¼Œä¸”åªè¿”å›ä¸€ä¸ªåˆ—å€¼
  
  ```sql
  SELECT col1,
         COALESCE ((SELECT Max(col2)
                    FROM   t1
                    WHERE  t1.col1 = t2.col1), 0) AS col2
  FROM   t2;                        -- è½¬ä¸ºjoin
  ```
  
  

package org.apache.spark.sql.execution

SparkPlanä¸­ä½¿ç”¨çš„å­æŸ¥è¯¢

```scala
case class ScalarSubquery(
    plan: BaseSubqueryExec,
    exprId: ExprId)
  extends ExecSubqueryExpression

abstract class ExecSubqueryExpression extends PlanExpression

abstract class PlanExpression[T <: QueryPlan[_]] extends Expression
```





package org.apache.spark.sql.catalyst.expressions

```scala
case class ScalarSubquery(
    plan: LogicalPlan,
    outerAttrs: Seq[Expression] = Seq.empty,
    exprId: ExprId = NamedExpression.newExprId,
    joinCond: Seq[Expression] = Seq.empty)
  extends SubqueryExpression(plan, outerAttrs, exprId, joinCond)


abstract class SubqueryExpression(
    plan: LogicalPlan,
    outerAttrs: Seq[Expression],
    exprId: ExprId,
    joinCond: Seq[Expression] = Nil) extends PlanExpression[LogicalPlan]
```





# ScalarSubqueryçš„ä¸€ä¸ªä¾‹å­ğŸŒ°

select *  from p where age=(select age from p where age=30)

```mathematica
== Parsed Logical Plan ==
'Project [*]
+- 'Filter ('age = scalar-subquery#12 [])
   :  +- 'Project ['age]
   :     +- 'Filter ('age = 30)
   :        +- 'UnresolvedRelation [p], [], false
   +- 'UnresolvedRelation [p], [], false

== Analyzed Logical Plan ==
age: bigint, name: string
Project [age#8L, name#9]
+- Filter (age#8L = scalar-subquery#12 [])
   :  +- Project [age#13L]
   :     +- Filter (age#13L = cast(30 as bigint))
   :        +- SubqueryAlias p
   :           +- View (`p`, [age#13L,name#14])
   :              +- Relation [age#13L,name#14] json
   +- SubqueryAlias p
      +- View (`p`, [age#8L,name#9])
         +- Relation [age#8L,name#9] json

== Optimized Logical Plan ==
é€»è¾‘è®¡åˆ’è¿™é‡Œscalar-subqueryæ˜¯catalyst.expressionsåŒ…ä¸‹é¢çš„SubqueryExpressionçš„å­ç±»
Filter (isnotnull(age#8L) AND (age#8L = scalar-subquery#12 []))
:  +- Project [age#13L]
:     +- Filter (isnotnull(age#13L) AND (age#13L = 30))
:        +- Relation [age#13L,name#14] json
+- Relation [age#8L,name#9] json

== Physical Plan ==
åˆ°äº†ç‰©ç†è®¡åˆ’è¿™é‡Œï¼Œscalar-subqueryæ˜¯executionåŒ…ä¸‹é¢çš„ExecSubqueryExpressionçš„å­ç±»
AdaptiveSparkPlan isFinalPlan=false
+- Filter (isnotnull(age#8L) AND (age#8L = Subquery subquery#12, [id=#18]))
   :  +- Subquery subquery#12, [id=#18]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- Filter (isnotnull(age#13L) AND (age#13L = 30))
   :           +- FileScan json [age#13L] Batched: false, DataFilters: [isnotnull(age#13L), (age#13L = 30)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/chengxingfu/code/open/spark/sourceCode/spark/examples/src/..., PartitionFilters: [], PushedFilters: [IsNotNull(age), EqualTo(age,30)], ReadSchema: struct<age:bigint>
   +- FileScan json [age#8L,name#9] Batched: false, DataFilters: [isnotnull(age#8L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/chengxingfu/code/open/spark/sourceCode/spark/examples/src/..., PartitionFilters: [], PushedFilters: [IsNotNull(age)], ReadSchema: struct<age:bigint,name:string>

```

æ’ä»¶æ˜¯ä¿®æ”¹çš„ç‰©ç†è®¡åˆ’æ ‘ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆè°ƒæŸ¥executionåŒ…ä¸‹çš„ScalarSubquery



SparkPlançš„executeQueryæ–¹æ³•æºç ï¼š

```scala
  protected final def executeQuery[T](query: => T): T = {
    RDDOperationScope.withScope(sparkContext, nodeName, false, true) {
      prepare()
      waitForSubqueries()
      query
    }
  }
```

é¦–å…ˆè¿›è¡Œprepare,å†waitForSubqueriesï¼› å…¶ä¸­prepareä¼šæŠŠæ‰€æœ‰ExecSubqueryExpressionç±»å‹çš„è¡¨è¾¾å¼å½“ä½œsubqueryæ”¶é›†èµ·æ¥ï¼ŒwaitForSubqueriesä¼šæŠŠæ”¶é›†åˆ°çš„subqueryæäº¤æ‰§è¡Œã€‚

æœ¬ä¾‹ä¸­ï¼ŒScalarSubqueryå‡ºç°åœ¨Filterç®—å­ä¸­ï¼ŒFilterç®—å­åœ¨æ‰§è¡Œæ—¶ä¼šwaitForSubqueriesï¼Œå®ƒä¼šè°ƒç”¨ScalarSubqueryçš„updateResultæ–¹æ³•

![image-20230912171902049](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230912171902049.png)ï¼Œ è¿™ä¼šè§¦å‘å­æŸ¥è¯¢çš„æ‰§è¡Œ, æ›´é‡è¦çš„æ˜¯è·å–åˆ°äº†è¯¥è¡¨è¾¾å¼çš„æ‰§è¡Œç»“æœresultï¼Œä»è€Œè¯¥è¡¨è¾¾å¼è®¡ç®—æ—¶åªéœ€è¦æŠŠè¿™ä¸ªresultè¿”å›å³å¯ã€‚

```scala
// case class ScalarSubquery
def updateResult(): Unit = {
    //è§¦å‘ScalarSubqueryçš„æ‰§è¡Œ
    val rows = plan.executeCollect()
    if (rows.length > 1) {
      sys.error(s"more than one row returned by a subquery used as an expression:\n$plan")
    }
    if (rows.length == 1) {
      assert(rows(0).numFields == 1,
        s"Expects 1 field, but got ${rows(0).numFields}; something went wrong in analysis")
      result = rows(0).get(0, dataType)
    } else {
      // If there is no rows returned, the result should be null.
      result = null
    }
    updated = true
  }
```



è°ƒè¯•æ—¶æ³¨æ„ï¼Œæ‰“å¼€codegenåï¼Œæœ‰çš„æ¥å£å¯èƒ½ä¸ä¼šèµ°åˆ°



[reuse adaptive subquery](https://www.waitingforcode.com/apache-spark-sql/what-new-apache-spark-3-reuse-adaptive-subquery/read)

[Spark-SQLä¸­å…³äºSubqueryçš„å¤„ç†](https://bbs.huaweicloud.com/blogs/189426)

# Sparkå¯¹å­æŸ¥è¯¢çš„è®¾è®¡

å­æŸ¥è¯¢é€šå¸¸æŒ‡åµŒå¥—åœ¨ä¸€ä¸ªæŸ¥è¯¢å†…éƒ¨çš„å®Œæ•´æŸ¥è¯¢ï¼Œå¸¸è§çš„æ˜¯ä½œä¸ºæ•°æ®æºå‡ºç°åœ¨SQLçš„FROMå…³é”®å­—ä¹‹åã€‚ Spark2.0ç‰ˆæœ¬ä¹‹ååˆæ”¯æŒäº†ä¸¤ç§ç‰¹æ®Šçš„å­æŸ¥è¯¢ï¼Œå³Scalarç±»å‹å’ŒInSubqueryç±»å‹

1. Scalarç±»å‹çš„å­æŸ¥è¯¢è¿”å›å•ä¸ªå€¼ï¼Œå…·ä½“åˆåˆ†ç›¸å…³ç±»å‹(Correlated)å’Œä¸ç›¸å…³ç±»å‹(Uncorrelated)ã€‚ Uncorrelatedæ„å‘³ç€å­æŸ¥è¯¢å’Œä¸»æŸ¥è¯¢ä¸å­˜åœ¨ç›¸å…³æ€§ï¼ŒUncorrelatedä¼šåœ¨ä¸»æŸ¥è¯¢ä¹‹å‰æ‰§è¡Œã€‚
2. InSubqueryç±»å‹è¡¨ç¤ºå­æŸ¥è¯¢ä½œä¸ºè¿‡æ»¤è°“è¯ï¼Œå¯ä»¥å‡ºç°åœ¨EXISTSå’ŒINè¯­å¥ä¸­

é€šè¿‡ä¸‹é¢çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°

1. FROMå…³é”®å­—ä¹‹åçš„åµŒå¥—æŸ¥è¯¢åªåœ¨è§£ææ—¶èµ·åˆ°ä¸€ä¸ªå‰åç®—å­çš„æ¡¥æ¥ä½œç”¨ï¼Œä¼˜åŒ–åæ¶ˆå¤±
2. InSubqueryç±»å‹çš„å­æŸ¥è¯¢è¢«è§„åˆ™RewritePredicateSubqueryæ”¹ä¸ºleft semi/anti join
3. Correlated Scalarç±»å‹çš„å­æŸ¥è¯¢ç”±è§„åˆ™RewriteCorrelatedScalarSubqueryé‡å†™ä¸ºleft outer join
4. æˆ‘ä»¬åªéœ€å¤„ç†Uncorrelated Scalarç±»å‹çš„å­æŸ¥è¯¢

## SubqueryExpression

### ä¾èµ–å…³ç³»

è¡¨è¾¾å¼ç»§æ‰¿å…³ç³»ï¼š

![image-20230925093549509](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230925093549509.png)

å·¦ä¾§åœ¨executionåŒ…(è¡¨ç¤ºç‰©ç†è®¡åˆ’)ï¼Œå³ä¾§åœ¨expressionsåŒ…ï¼ˆè¡¨ç¤ºé€»è¾‘è®¡åˆ’ï¼‰;  ç‰©ç†è®¡åˆ’ä¸­çš„ScalarSubqueryå’ŒInSubqueryExecè¡¨è¾¾å¼éƒ½ä¾èµ–ç®—å­æ¥è®¡ç®—ç»“æœ(å…·ä½“å°±æ˜¯æ‰§è¡Œç®—å­çš„executeCollect()æ–¹æ³•)ï¼Œä»–ä»¬ä¾èµ–çš„ç®—å­çš„ä¾èµ–å…³ç³»å¦‚ä¸‹ï¼š

<img src="https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20231007154632656.png" alt="image-20231007154632656" style="zoom:50%;" />







### SubqueryExpressionå®ç°ç±»

åŒ…å«ä¸€ä¸ªé€»è¾‘è®¡åˆ’çš„è¡¨è¾¾å¼

æœ‰5ä¸ªå®ç°ç±»

1. ScalarSubquery: åªè¿”å›å•è¡Œå•åˆ—çš„å­æŸ¥è¯¢

2. Existsï¼š è¢«é‡å†™æˆleft semi/anti join

3. DynamicPruningSubquery

   ä½¿ç”¨joinä¸€è¾¹çš„filterè¿‡æ»¤å¦ä¸€è¾¹çš„è¡¨ï¼Œåœ¨åº”ç”¨åˆ†åŒºè£å‰ªæ—¶æ’å…¥çš„

4. LateralSubquery:  è¿”å›å¤šè¡Œ/åˆ—çš„å­æŸ¥è¯¢ï¼Œä¼˜åŒ–é˜¶æ®µä¼šè¢«RewriteLateralSubqueryé‡å†™ä¸ºjoin 

5. ListQuery
   åªèƒ½å’Œinè¡¨è¾¾å¼ä¸€èµ·ä½¿ç”¨ï¼Œä½œä¸ºinçš„æŸ¥è¯¢èŒƒå›´ï¼Œåªå­˜åœ¨äºé€»è¾‘è®¡åˆ’ä¸­
   
   ```sql
   SELECT  *
   FROM    a
   WHERE   a.id IN (SELECT  id
                    FROM    b)
   ```
   
   å®éªŒ
   
   ```sql
   select  * from ds1g.inventory where ds1g.inventory.inv_item_sk in (select i_item_sk from ds1g.item)
   ```
   
   è®¡åˆ’æ ‘ï¼š  é€»è¾‘è®¡åˆ’ä¸­ListQueryè¢«ä¼˜åŒ–ä¸ºleft semi join
   
   ```json
   == Analyzed Logical Plan ==
   inv_date_sk: int, inv_item_sk: int, inv_warehouse_sk: int, inv_quantity_on_hand: int
   Project [inv_date_sk#1, inv_item_sk#2, inv_warehouse_sk#3, inv_quantity_on_hand#4]
   +- Filter inv_item_sk#2 IN (list#0 [])
      :  +- Project [i_item_sk#5]
      :     +- SubqueryAlias spark_catalog.ds1g.item
      :        +- Relation ds1g.item[i_item_sk#5,i_item_id#6,i_rec_start_date#7,i_rec_end_date#8,i_item_desc#9,i_current_price#10,i_wholesale_cost#11,i_brand_id#12,i_brand#13,i_class_id#14,i_class#15,i_category_id#16,i_category#17,i_manufact_id#18,i_manufact#19,i_size#20,i_formulation#21,i_color#22,i_units#23,i_container#24,i_manager_id#25,i_product_name#26] parquet
      +- SubqueryAlias spark_catalog.ds1g.inventory
         +- Relation ds1g.inventory[inv_date_sk#1,inv_item_sk#2,inv_warehouse_sk#3,inv_quantity_on_hand#4] parquet
   
   == Optimized Logical Plan ==
   Join LeftSemi, (inv_item_sk#2 = i_item_sk#5)
   :- Relation ds1g.inventory[inv_date_sk#1,inv_item_sk#2,inv_warehouse_sk#3,inv_quantity_on_hand#4] parquet
   +- Project [i_item_sk#5]
      +- Relation ds1g.item[i_item_sk#5,i_item_id#6,i_rec_start_date#7,i_rec_end_date#8,i_item_desc#9,i_current_price#10,i_wholesale_cost#11,i_brand_id#12,i_brand#13,i_class_id#14,i_class#15,i_category_id#16,i_category#17,i_manufact_id#18,i_manufact#19,i_size#20,i_formulation#21,i_color#22,i_units#23,i_container#24,i_manager_id#25,i_product_name#26] parquet
   
   == Physical Plan ==
   AdaptiveSparkPlan isFinalPlan=false
   +- BroadcastHashJoin [inv_item_sk#2], [i_item_sk#5], LeftSemi, BuildRight, false
      :- FileScan parquet ds1g.inventory[inv_date_sk#1,inv_item_sk#2,inv_warehouse_sk#3,inv_quantity_on_hand#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/inventory], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<inv_date_sk:int,inv_item_sk:int,inv_warehouse_sk:int,inv_quantity_on_hand:int>
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#18]
         +- FileScan parquet ds1g.item[i_item_sk#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i_item_sk:int>
   
   ```
   

<font color=red>çœ‹èµ·æ¥åªæœ‰ScalarSubqueryå’ŒDynamicPruningSubqueryä¸ä¼šè¢«æ›¿æ¢ä¸ºJoin,  ä»ä¾èµ–å…³ç³»å›¾åˆ†æInSubqueryExecåº”è¯¥å°±æ˜¯ç”¨äºDynamic Partition Pruning</font> ï¼Œ[è¿™é‡Œ](https://git.odin.cse.buffalo.edu/ODIn/spark-instrumented-optimizer/commit/d2a5dad97c0cd9c2b7eede72a0a8df268714155f)ä¹Ÿè¯´å®ƒåªç”¨äºDPPã€‚

### ScalarSubquery(executionåŒ…)

ä»ç­¾åçœ‹ï¼Œä»–æ˜¯ä¸€ä¸ªè¡¨è¾¾å¼ï¼Œåªæ˜¯è¿™ä¸ªè¡¨è¾¾å¼å†…éƒ¨æœ‰ä¸€ä¸ªBaseSubqueryExecå‹çš„ç®—å­ï¼Œè¯¥ç®—å­æœ‰å››ä¸ªå®ç°ç±»ï¼ŒScalarSubqueryåœ¨æ„é€ æ—¶

![image-20230927105143699](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230927105143699.png)

ä¸€èˆ¬æ˜¯SubqueryExec(AQEå¼€å¯åï¼Œå¯èƒ½æ˜¯å…¶ä»–ç±»å‹ï¼Œç”±è§„åˆ™PlanAdaptiveSubqueriesæ¥åˆ›å»º)

```scala
        ScalarSubquery(
          SubqueryExec.createForScalarSubquery(
            s"scalar-subquery#${subquery.exprId.id}", executedPlan),
          subquery.exprId)
```

SubqueryExecå¹¶ä¸å…è®¸è°ƒç”¨doExecute()ï¼Œè€Œåªå…è®¸è°ƒç”¨executeCollect()ï¼Œä»è€Œæäº¤jobï¼Œè¿”å›æ•°æ®ã€‚

æ—¢ç„¶ScalarSubqueryæ˜¯è¡¨è¾¾å¼ï¼Œå…¥å£å°±æ˜¯evalå‡½æ•°ï¼Œevalå¾ˆç®€å•ï¼Œç›´æ¥æŠŠresultè¿”å›ã€‚ä½†æ˜¯è¦å…ˆç­‰å¾…updateResultè®¡ç®—å®Œã€‚è¿™ä¸ªupdateResultç”±waitForSubqueriesè§¦å‘ã€‚

```scala
case class ScalarSubquery(
    plan: BaseSubqueryExec,
    exprId: ExprId)
  extends ExecSubqueryExpression with LeafLike[Expression] {

  override def dataType: DataType = plan.schema.fields.head.dataType
  override def nullable: Boolean = true
  override def toString: String = plan.simpleString(SQLConf.get.maxToStringFields)
  override def withNewPlan(query: BaseSubqueryExec): ScalarSubquery = copy(plan = query)

  override lazy val preCanonicalized: Expression = {
    ScalarSubquery(plan.canonicalized.asInstanceOf[BaseSubqueryExec], ExprId(0))
  }

  // the first column in first row from `query`.
  @volatile private var result: Any = _
  @volatile private var updated: Boolean = false

  def updateResult(): Unit = {
    val rows = plan.executeCollect()
    if (rows.length > 1) {
      sys.error(s"more than one row returned by a subquery used as an expression:\n$plan")
    }
    if (rows.length == 1) {
      assert(rows(0).numFields == 1,
        s"Expects 1 field, but got ${rows(0).numFields}; something went wrong in analysis")
      result = rows(0).get(0, dataType)
    } else {
      // If there is no rows returned, the result should be null.
      result = null
    }
    updated = true
  }

  override def eval(input: InternalRow): Any = {
    require(updated, s"$this has not finished")
    result
  }

  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    require(updated, s"$this has not finished")
    Literal.create(result, dataType).doGenCode(ctx, ev)
  }
}
```



### InSubqueryExec

#### InSubquery Exepression(é€»è¾‘è®¡åˆ’)

ä»£è¡¨é€»è¾‘è®¡åˆ’ä¸­çš„INåˆ¤æ–­(å®éªŒå‘ç°è¿™ç§æƒ…å†µè²Œä¼¼å·²ç»è¢«ListQueryç»™ä»£æ›¿äº†)

```sql
NOT? IN '(' query ')'
```

ä¹Ÿå¯ä»¥ç”¨åœ¨å…¶ä»–åœºæ™¯ï¼Œä¾‹å¦‚Runtime Filtering,  Dynamic Partition Pruning

```scala
case class InSubquery(values: Seq[Expression], query: ListQuery)
  extends Predicate with Unevaluable {
```

åˆ›å»ºåœºæ™¯ï¼š

- [InjectRuntimeFilter](https://books.japila.pl/spark-sql-internals/logical-optimizations/InjectRuntimeFilter/) logical optimization is executed (and [injectInSubqueryFilter](https://books.japila.pl/spark-sql-internals/logical-optimizations/InjectRuntimeFilter/#injectInSubqueryFilter))
- `AstBuilder` is requested to [withPredicate](https://books.japila.pl/spark-sql-internals/sql/AstBuilder/#withPredicate) (for `NOT? IN '(' query ')'` SQL predicate)
- [PlanDynamicPruningFilters](https://books.japila.pl/spark-sql-internals/physical-optimizations/PlanDynamicPruningFilters/) physical optimization is executed (with [spark.sql.optimizer.dynamicPartitionPruning.enabled](https://books.japila.pl/spark-sql-internals/configuration-properties/#spark.sql.optimizer.dynamicPartitionPruning.enabled) enabledï¼Œé»˜è®¤true)
- `RowLevelOperationRuntimeGroupFiltering` logical optimization is executed

##### Unevaluable

InSubqueryä¸å¯æ‰§è¡Œï¼š

- [RewritePredicateSubquery](https://books.japila.pl/spark-sql-internals/logical-optimizations/RewritePredicateSubquery/): åœ¨é€»è¾‘è®¡åˆ’ä¼˜åŒ–æ—¶è½¬åŒ–ä¸ºleft-semi  join(NOT IN æ—¶è½¬ä¸ºleft-anti join)
- [PlanSubqueries](https://books.japila.pl/spark-sql-internals/physical-optimizations/PlanSubqueries/) : ç‰©ç†è®¡åˆ’ä¼˜åŒ–æ—¶è½¬ä¸ºSubqueryExecä¸Šçš„ä¸€ä¸ªInSubqueryExecè¡¨è¾¾å¼

#### InSubqueryExec Exepression(ç‰©ç†è®¡åˆ’)

ä»£è¡¨InSubquery è¡¨è¾¾å¼å’Œ[DynamicPruningSubquery](https://books.japila.pl/spark-sql-internals/expressions/DynamicPruningSubquery/)è¡¨è¾¾å¼çš„ç‰©ç†è®¡åˆ’

```scala
case class InSubqueryExec(
    child: Expression,
    plan: BaseSubqueryExec,
    exprId: ExprId,
    shouldBroadcast: Boolean = false,
    private var resultBroadcast: Broadcast[Array[Any]] = null,
    @transient private var result: Array[Any] = null)
```



åˆ›å»ºåœºæ™¯

- [PlanSubqueries](https://books.japila.pl/spark-sql-internals/physical-optimizations/PlanSubqueries/) physical optimization is executed (and plans [InSubquery](https://books.japila.pl/spark-sql-internals/expressions/InSubquery/) expressions)
- [PlanAdaptiveSubqueries](https://books.japila.pl/spark-sql-internals/physical-optimizations/PlanAdaptiveSubqueries/) physical optimization is executed (and plans [InSubquery](https://books.japila.pl/spark-sql-internals/expressions/InSubquery/) expressions)
- [PlanDynamicPruningFilters](https://books.japila.pl/spark-sql-internals/physical-optimizations/PlanDynamicPruningFilters/) physical optimization is executed (and plans [DynamicPruningSubquery](https://books.japila.pl/spark-sql-internals/expressions/DynamicPruningSubquery/) expressions)





## FROMåçš„å­æŸ¥è¯¢

å®éªŒè¯­å¥

```sql
select max(sk),brd from (select i_item_sk as sk,i_brand as brd from item) group by brd
```

è®¡åˆ’æ ‘ï¼šå¯ä»¥å‘ç°FROMåçš„å­æŸ¥è¯¢åªåœ¨è§£ææ—¶èµ·åˆ°ä¸€ä¸ªå‰åç®—å­çš„æ¡¥æ¥ä½œç”¨ï¼Œä¼˜åŒ–åå·²ç»æ¶ˆå¤±ã€‚

```json
== Parsed Logical Plan ==
'Aggregate ['brd], [unresolvedalias('max('sk), None), 'brd]
+- 'SubqueryAlias __auto_generated_subquery_name
   +- 'Project ['i_item_sk AS sk#0, 'i_brand AS brd#1]
      +- 'UnresolvedRelation [item], [], false

== Analyzed Logical Plan ==
max(sk): int, brd: string
Aggregate [brd#1], [max(sk#0) AS max(sk)#25, brd#1]
+- SubqueryAlias __auto_generated_subquery_name
   +- Project [i_item_sk#2 AS sk#0, i_brand#10 AS brd#1]
      +- SubqueryAlias spark_catalog.ds1g.item
         +- Relation ds1g.item[i_item_sk#2,i_item_id#3,i_rec_start_date#4,i_rec_end_date#5,i_item_desc#6,i_current_price#7,i_wholesale_cost#8,i_brand_id#9,i_brand#10,i_class_id#11,i_class#12,i_category_id#13,i_category#14,i_manufact_id#15,i_manufact#16,i_size#17,i_formulation#18,i_color#19,i_units#20,i_container#21,i_manager_id#22,i_product_name#23] parquet

== Optimized Logical Plan ==
Aggregate [brd#1], [max(sk#0) AS max(sk)#25, brd#1]
+- Project [i_item_sk#2 AS sk#0, i_brand#10 AS brd#1]
   +- Relation ds1g.item[i_item_sk#2,i_item_id#3,i_rec_start_date#4,i_rec_end_date#5,i_item_desc#6,i_current_price#7,i_wholesale_cost#8,i_brand_id#9,i_brand#10,i_class_id#11,i_class#12,i_category_id#13,i_category#14,i_manufact_id#15,i_manufact#16,i_size#17,i_formulation#18,i_color#19,i_units#20,i_container#21,i_manager_id#22,i_product_name#23] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[brd#1], functions=[max(sk#0)], output=[max(sk)#25, brd#1])
   +- Exchange hashpartitioning(brd#1, 200), ENSURE_REQUIREMENTS, [id=#16]
      +- HashAggregate(keys=[brd#1], functions=[partial_max(sk#0)], output=[brd#1, max#51])
         +- Project [i_item_sk#2 AS sk#0, i_brand#10 AS brd#1]
            +- FileScan parquet ds1g.item[i_item_sk#2,i_brand#10] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i_item_sk:int,i_brand:string>
```

```sql
select sk_cnt, count(1) from (select max(i_item_sk) as sk_cnt,i_brand as brd from item group by brd) group by sk_cnt
```

è®¡åˆ’æ ‘

```json
== Parsed Logical Plan ==
'Aggregate ['sk_cnt], ['sk_cnt, unresolvedalias('count(1), None)]
+- 'SubqueryAlias __auto_generated_subquery_name
   +- 'Aggregate ['brd], ['max('i_item_sk) AS sk_cnt#0, 'i_brand AS brd#1]
      +- 'UnresolvedRelation [item], [], false

== Analyzed Logical Plan ==
sk_cnt: int, count(1): bigint
Aggregate [sk_cnt#0], [sk_cnt#0, count(1) AS count(1)#26L]
+- SubqueryAlias __auto_generated_subquery_name
   +- Aggregate [i_brand#11], [max(i_item_sk#3) AS sk_cnt#0, i_brand#11 AS brd#1]
      +- SubqueryAlias spark_catalog.ds1g.item
         +- Relation ds1g.item[i_item_sk#3,i_item_id#4,i_rec_start_date#5,i_rec_end_date#6,i_item_desc#7,i_current_price#8,i_wholesale_cost#9,i_brand_id#10,i_brand#11,i_class_id#12,i_class#13,i_category_id#14,i_category#15,i_manufact_id#16,i_manufact#17,i_size#18,i_formulation#19,i_color#20,i_units#21,i_container#22,i_manager_id#23,i_product_name#24] parquet

== Optimized Logical Plan ==
Aggregate [sk_cnt#0], [sk_cnt#0, count(1) AS count(1)#26L]
+- Aggregate [i_brand#11], [max(i_item_sk#3) AS sk_cnt#0]
   +- Project [i_item_sk#3, i_brand#11]
      +- Relation ds1g.item[i_item_sk#3,i_item_id#4,i_rec_start_date#5,i_rec_end_date#6,i_item_desc#7,i_current_price#8,i_wholesale_cost#9,i_brand_id#10,i_brand#11,i_class_id#12,i_class#13,i_category_id#14,i_category#15,i_manufact_id#16,i_manufact#17,i_size#18,i_formulation#19,i_color#20,i_units#21,i_container#22,i_manager_id#23,i_product_name#24] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[sk_cnt#0], functions=[count(1)], output=[sk_cnt#0, count(1)#26L])
   +- Exchange hashpartitioning(sk_cnt#0, 200), ENSURE_REQUIREMENTS, [id=#25]
      +- HashAggregate(keys=[sk_cnt#0], functions=[partial_count(1)], output=[sk_cnt#0, count#52L])
         +- HashAggregate(keys=[i_brand#11], functions=[max(i_item_sk#3)], output=[sk_cnt#0])
            +- Exchange hashpartitioning(i_brand#11, 200), ENSURE_REQUIREMENTS, [id=#21]
               +- HashAggregate(keys=[i_brand#11], functions=[partial_max(i_item_sk#3)], output=[i_brand#11, max#54])
                  +- FileScan parquet ds1g.item[i_item_sk#3,i_brand#11] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/hive/warehouse/ds1g.db/item], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i_item_sk:int,i_brand:string>

```





## å­æŸ¥è¯¢è§£æ

å­æŸ¥è¯¢åˆ†ä¸ºCTEé‡Œçš„å­æŸ¥è¯¢å’Œè¡¨è¾¾å¼é‡Œçš„å­æŸ¥è¯¢ï¼ŒCTEé‡Œçš„å­æŸ¥è¯¢ç”±CTEåœºæ™¯å¤„ç†ï¼Œè¿™é‡ŒæŒ‡è¡¨è¾¾å¼é‡Œçš„å­æŸ¥è¯¢

### ResolveSubquery



## é€»è¾‘è®¡åˆ’ä¼˜åŒ–

### OptimizeSubqueriesä¼˜åŒ–è§„åˆ™

æ˜¯Subquery Batché‡Œçš„å”¯ä¸€çš„è§„åˆ™ã€‚å½“SQLè¯­å¥åŒ…å«å­æŸ¥è¯¢æ—¶ï¼Œä¼šåœ¨é€»è¾‘ç®—å­æ ‘ä¸Šç”ŸæˆSubqueryExpressionè¡¨è¾¾å¼ï¼Œè¿™ä¸ªè§„åˆ™é€’å½’å¯¹SubqueryExpressionè¡¨è¾¾å¼çš„å­è®¡åˆ’è°ƒç”¨Optimizerå¹¶è¿›è¡Œä¼˜åŒ–ã€‚

```scala
  object OptimizeSubqueries extends Rule[LogicalPlan] {
    private def removeTopLevelSort(plan: LogicalPlan): LogicalPlan = {
      if (!plan.containsPattern(SORT)) {
        return plan
      }
      plan match {
        case Sort(_, _, child) => child
        case Project(fields, child) => Project(fields, removeTopLevelSort(child))
        case other => other
      }
    }
    def apply(plan: LogicalPlan): LogicalPlan = plan.transformAllExpressionsWithPruning(
      _.containsPattern(PLAN_EXPRESSION), ruleId) {
      case s: SubqueryExpression =>
      //å¯¹å­æŸ¥è¯¢è¡¨è¾¾å¼é‡Œçš„è®¡åˆ’æ ‘è¿›è¡Œä¼˜åŒ–ï¼Œä¸¢æ‰æ’åºç­‰
        val Subquery(newPlan, _) = Optimizer.this.execute(Subquery.fromExpression(s))
        // At this point we have an optimized subquery plan that we are going to attach
        // to this subquery expression. Here we can safely remove any top level sort
        // in the plan as tuples produced by a subquery are un-ordered.
        s.withNewPlan(removeTopLevelSort(newPlan))
    }
  }
```



### RewritePredicateSubqueryè§„åˆ™

æŠŠInSubqueryç±»å‹çš„å­æŸ¥è¯¢æ”¹å†™æˆleft semi/anti joinï¼Œæ‰€ä»¥è¿™ç§å­æŸ¥è¯¢æ— éœ€æˆ‘ä»¬å¸è½½ã€‚ä¾‹å¦‚

EXISTS/NOT EXISTS å’Œ IN/NOT IN

1. Correlated condition

```sql
SELECT  *
FROM    a
WHERE   EXISTS (SELECT  *
                FROM    b
                WHERE   b.id = a.id)
```

2. Uncorrelated codition

```sql
SELECT  *
FROM    a
WHERE   EXISTS (SELECT  *
                FROM    b
                WHERE   b.id > 10)
```



### RewriteCorrelatedScalarSubqueryè§„åˆ™

ä¼šæŠŠCorrelatedç±»å‹çš„ScalarSubqueryé‡å†™ä¸ºleft outer join

## é€»è¾‘è®¡åˆ’è½¬ç‰©ç†è®¡åˆ’

### PlanSubqueriesè§„åˆ™


sparkåœ¨prepareForExecutionè·å¾—å¯æ‰§è¡Œç‰©ç†è®¡åˆ’æ—¶ä¼šåº”ç”¨ä¸€æ‰¹è§„åˆ™ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªè§„åˆ™PlanSubqueriesï¼Œä¼šå¯¹Scalarubqueryä»¥åŠInSubqueryè¿›ä¸€æ­¥å¤„ç†ã€‚ä¼šå°†subqueryä¸­çš„planï¼Œå†ä¸€æ¬¡ç»è¿‡parserã€analyzerã€optimizerå¾—åˆ°ç‰©ç†æ‰§è¡Œè®¡åˆ’(executedPlan)ï¼Œç„¶åå°è£…ä¸ºScalarSubqueryå’ŒInSubqueryExecç®—å­å‹è¡¨è¾¾å¼ã€‚

```scala
case class PlanSubqueries(sparkSession: SparkSession) extends Rule[SparkPlan] {
  def apply(plan: SparkPlan): SparkPlan = {
    //å¯¹å½“å‰ç®—å­ä¸‹çš„è¡¨è¾¾å¼è¿›è¡Œæ›¿æ¢
    plan.transformAllExpressionsWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY, IN_SUBQUERY)) {
      case subquery: expressions.ScalarSubquery =>
        //1)å¦‚æœæ˜¯ä¸€ä¸ªScalarSubqueryè¡¨è¾¾å¼ï¼Œé‚£ä¹ˆå…ˆæ‹¿åˆ°å®ƒçš„é€»è¾‘è®¡åˆ’ï¼Œ æ¥ç€è½¬æ¢æˆç‰©ç†è®¡åˆ’ä»¥åŠprepareExecutedPlanï¼Œå¾—åˆ°å¯æ‰§è¡Œç‰©ç†è®¡åˆ’
        val executedPlan = QueryExecution.prepareExecutedPlan(sparkSession, subquery.plan)
          //3)æŠŠSubqueryExecç®—å­åŒ…è£…æˆä¸€ä¸ªè¡¨è¾¾å¼
        ScalarSubquery(
              //2)åœ¨ç‰©ç†è®¡åˆ’ä¸ŠåŠ ä¸€ä¸ªSubqueryExecç®—å­
          SubqueryExec.createForScalarSubquery(
            s"scalar-subquery#${subquery.exprId.id}", executedPlan),
          subquery.exprId)
      case expressions.InSubquery(values, ListQuery(query, _, exprId, _, _)) =>
          //InSubqueryè¡¨è¾¾å¼
        val expr = if (values.length == 1) {
          values.head
        } else {
          CreateNamedStruct(
            values.zipWithIndex.flatMap { case (v, index) =>
              Seq(Literal(s"col_$index"), v)
            }
          )
        }
          // åŒæ ·æŠŠå­æŸ¥è¯¢åˆ›å»ºç‰©ç†è®¡åˆ’
        val executedPlan = QueryExecution.prepareExecutedPlan(sparkSession, query)
         //  æŠŠç‰©ç†è®¡åˆ’ä¸ŠåŠ ä¸€ä¸ªsubQueryExecç®—å­ï¼Œ å†å°è£…ä¸ºä¸€ä¸ªè¡¨è¾¾å¼
        InSubqueryExec(expr, SubqueryExec(s"subquery#${exprId.id}", executedPlan), exprId)
    }
  }
}
```



# tpcds-queryä¸­çš„subquery

## q06ç‰©ç†è®¡åˆ’ä¸­çš„ScalarSubquery

```sql
spark.sql("""
SELECT state, cnt FROM (
 SELECT a.ca_state state, count(*) cnt
 FROM
    customer_address a, customer c, store_sales s, date_dim d, item i
 WHERE a.ca_address_sk = c.c_current_addr_sk
   AND c.c_customer_sk = s.ss_customer_sk
   AND s.ss_sold_date_sk = d.d_date_sk
   AND s.ss_item_sk = i.i_item_sk
   AND d.d_month_seq =
      (SELECT distinct (d_month_seq) FROM date_dim
        WHERE d_year = 2001 AND d_moy = 1)
   AND i.i_current_price > 1.2 *
             (SELECT avg(j.i_current_price) FROM item j
               WHERE j.i_category = i.i_category)
 GROUP BY a.ca_state
) x
WHERE cnt >= 10
ORDER BY cnt LIMIT 100
""").collect()
```

1. æ¡ä»¶  (SELECT avg(j.i_current_price) FROM item j
                  WHERE j.i_category = i.i_category)  æ˜¯ä¸€ä¸ªCorelated ScalarSubqueryï¼Œé€»è¾‘è®¡åˆ’é˜¶æ®µè¢«æ”¹å†™ä¸ºJoinäº†ã€‚

2. æ¡ä»¶

```sql
d.d_month_seq =
      (SELECT distinct (d_month_seq) FROM date_dim
        WHERE d_year = 2001 AND d_moy = 1)
```

å¯¹åº”çš„ç‰©ç†è®¡åˆ’æ ‘å¦‚ä¸‹ï¼Œå­æŸ¥è¯¢ä½¿ç”¨ScalarSubqueryå°è£…ï¼Œå®ƒåœ¨è®¡åˆ’æ ‘ä¸­çš„æ–‡å­—æè¿°æ˜¯Subquery subquery#297ï¼Œå…¶ä¸­SubqueryæŒ‡çš„æ˜¯å®ƒåŒ…å«çš„SubqueryExec plançš„è¡¨ç¤ºï¼Œâ€œsubquery#297â€æŒ‡çš„æ˜¯è¯¥plançš„åå­—ã€‚

![image-20230925171610665](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20230925171610665.png)

å…¶ä¼šé˜»æ­¢Filterçš„å¸è½½ï¼š

<img src="https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20231123142430204.png" alt="image-20231123142430204" style="zoom:33%;" />



