1. [逻辑回归原理小结](https://www.cnblogs.com/pinard/p/6029432.html)——刘建平
2. [逻辑回归(logistics regression)原理-让你彻底读懂逻辑回归](https://mp.weixin.qq.com/s/66W_ykeakaoIilZcjDvNCA)
3. [逻辑回归](https://zhuanlan.zhihu.com/p/74874291)

2. [逻辑归回面试题](https://zhuanlan.zhihu.com/p/299612493)，[逻辑回归常见面试点总结](https://www.cnblogs.com/modifyrong/p/7739955.html)

# 总结

1. 首先说线性模型，其原理是求出特征矩阵X到目标特征Y之间线性关系系数，这里目标特征Y是连续的，所以是回归模型；但是如果目标特征是离散的，能否用线性模型呢？当然也可以，一个办法就是对Y再做一次函数转换，变为g(Y)。如果我们令𝑔(𝑌)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了

2. **模型**

   这个函数g在逻辑回归中我们一般取为sigmoid函数，形式如下：

   $$ 𝑔(𝑧)=\frac{1}{1+e^{-z}}$$

   　　　　它有一个非常好的性质，即当z趋于正无穷时，𝑔(𝑧)g(z)趋于1，而当z趋于负无穷时，𝑔(𝑧)g(z)趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质：

   　　　　𝑔′(𝑧)=𝑔(𝑧)(1−𝑔(𝑧))

   　　　　如果我们令𝑔(𝑧)中的z为：𝑧=𝑥𝜃，这样就得到了二元逻辑回归模型的一般形式：$$ h\theta(x)=\frac{1}{1+e^{-x\theta}}$$

   　　　　其中x为样本输入，ℎ𝜃(𝑥)为模型输出，可以理解为某一分类的概率大小

2. **损失函数**
线性模型使用误差的平方和来定义损失函数，但是目标为分类时不适合，可以用最大似然法来推导出损失函数。
   
假设概率分布是伯努利分布，根据伯努利分布的定义，其概率质量函数为：


​                𝑃(𝑦=1|𝑥,𝜃)=ℎ𝜃(𝑥)

　　　　𝑃(𝑦=0|𝑥,𝜃)=1−ℎ𝜃(𝑥)

　　　　 把这两个式子写成一个式子，就是：

　　　　$$𝑃(𝑦|𝑥,𝜃)=ℎ𝜃(𝑥)^𝑦(1−ℎ𝜃(𝑥))^{1−𝑦}$$

　　　　其中y的取值只能是0或者1。

　　　　得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数𝜃。

<img src="https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image-20220924081113703.png" alt="image-20220924081113703" style="zoom:50%;" />

4. 损失函数优化
   对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，拟牛顿法等。推导梯度下降 法中$\theta$每次迭代的公式

5. 正则化

逻辑回归的L1正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法。　逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似。

6. 多远逻辑回归

   实际上二元逻辑回归的模型和损失函数很容易推广到多元逻辑回归。比如总是认为某种类型为正值，其余为0值，这种方法为最常用的one-vs-rest，简称OvR.

   　　　　另一种多元逻辑回归的方法是Many-vs-Many(MvM)，它会选择一部分类别的样本和另一部分类别的样本来做逻辑回归二分类。最常用的是One-Vs-One（OvO）。OvO是MvM的特例。每次我们选择两类样本来做二元逻辑回归。

7. 总结

   逻辑回归尤其是二元逻辑回归是非常常见的模型，训练速度很快，虽然使用起来没有支持向量机（SVM）那么占主流，但是解决普通的分类问题是足够了，训练速度也比起SVM要快不少。如果你要理解机器学习分类算法，那么第一个应该学习的分类算法个人觉得应该是逻辑回归。理解了逻辑回归，其他的分类算法再学习起来应该没有那么难了。

8. <font color=red>优缺点</font> [参考](https://blog.csdn.net/qq_27782503/article/details/88778831)

优点：

- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。

- 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。

- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。

- 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。

- 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的**概率分数**，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

  缺点：

- 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有- - 样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
- 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
- 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

## <font color=red>思考</font>

### 为什么适合离散特征？

1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化；

总的来说，特征离散化以后起到了加快计算，简化模型和增加泛化能力的作用。

### 为什么不用平方误差

如果用平方误差，即$$L=\frac{(y-\hat{y})^2}{2}$$

那么梯度是$$\frac{\partial L}{\partial w}=(\hat{y}-y)\sigma'(w.x)x$$,  其中${\sigma}(w.x)'=w.x(1-w.x) $

Sigmoid 函数在输入较大时的导数是非常小的，从而导致梯度逐渐消失，收敛变慢，训练中途也可能因为梯度过小而提前终止训练(梯度消失)。

### 高相关特征会造成怎样的影响？

实质上将原来的特征分成了 100 份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 为什么要把高相关性特征去掉？ 

- 1.去掉高度相关的特征会让模型的可解释性更好
- 2.可以大大提高训练的速度。

### 为什么要做特征交叉？

- 逻辑回归模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。
- 基本特征可以认为是全局建模，组合特征更加精细，是个性化建模

### 正则化

- L1正则和L2正则

- - 相同点：都用于避免过拟合
  - 不同点：
    1.两者引入的关于模型参数的先验条件不一样，L1是拉普拉斯分布，L2是正态分布。
    2.L1偏向于使模型参数变得稀疏（但实际上并不那么容易），L2偏向于使模型每一个参数都很小，但是更加稠密，从而防止过拟合。

- <img src="https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/v2-676847ea71a2128d4bf32c147a8773c2_1440w.jpg" alt="img" style="zoom:50%;" />

- 上图中的模型是线性回归，有两个特征，要优化的参数分别是w1和w2，左图的正则化是L2，右图是L1。蓝色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是红色边界（就是正则化那部分），二者相交处，才是最优参数。可见右边的最优参数只可能在坐标轴上，所以就会出现0权重参数，使得模型稀疏。