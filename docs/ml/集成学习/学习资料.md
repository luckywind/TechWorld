[集成学习原理小结](https://www.cnblogs.com/pinard/p/6131423.html)

[集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)

[scikit-learn Adaboost类库使用小结](https://www.cnblogs.com/pinard/p/6136914.html)





[梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)

[scikit-learn GBDT调参小结](https://www.cnblogs.com/pinard/p/6143927.html)

[XGBoost原理小结](https://www.cnblogs.com/pinard/p/10979808.html)

[XGBoost类库使用小结](https://www.cnblogs.com/pinard/p/11114748.html)



# 集成学习

它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。

## 个体学习器

集成学习有两个主要的问题需要解决:

1. 第一是如何得到若干个个体学习器
   1. 同质：同一个种类的。器使用最多的模型是CART决策树和神经网络
      1. 存在依赖关系：个体学习器之间存在强依赖关系，一系列个体学习器基本都需要**串行生成**，代表算法是boosting系列算法
      2. 不存在依赖关系：个体学习器之间不存在强依赖关系，一系列个体学习器可以**并行生成**，代表算法是bagging和随机森林
   2. 不同质：不同种类的。
2. 第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。

## boosting

![img](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/1042406-20161204194331365-2142863547.png)

​       Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

　　Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。



## bagging

Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：

![img](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/1042406-20161204200000787-1988863729.png)

从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

　　　　对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstrap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

　　　　随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。

# 结合策略

我们假定我得到的T个弱学习器是{ℎ1,ℎ2,...ℎ𝑇}

1. 平均法

𝐻(𝑥)=∑𝑖=1𝑇𝑤𝑖ℎ𝑖(𝑥)

2. 投票法
3. 学习法

