[一文读懂随机森林的解释和实现](https://zhuanlan.zhihu.com/p/51165358) 该号有很多不错的文章

# 归一化与标准化

## 标准化(standardization,0均值/1方差)

是用来处理特征的

中心标准化（Z-score normalization）的结果是使所有特征的数值被转化成为均值为0、标准差为1的正态分布

公式为：(X-mean)/std  计算时对每个属性/每列分别进行。

将数据按期属性（按列进行）减去其均值，并处以其方差。得到的结果是，对于每个属性/每列来说所有数据都聚集在0附近，方差为1。

### 方法一:使用sklearn.preprocessing.StandardScaler类

使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据

```python
from sklearn import preprocessing
import numpy as np
X = np.array([[ 1., -1.,  2.],
              [ 2.,  0.,  0.],
              [ 0.,  1., -1.]])
scaler = preprocessing.StandardScaler().fit(X)
scaler
#StandardScaler(copy=True, with_mean=True, with_std=True)
#特征均值 
scaler.mean_                                      
array([ 1. ...,  0. ...,  0.33...])
#特征标准差 
scaler.scale_                                    
array([ 0.81...,  0.81...,  1.24...])
 
scaler.transform(X)                               
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])
# 标准化后的数据是0均值，1方差
X_scaled.mean(axis=0)
array([0., 0., 0.])
X_scaled.std(axis=0)
array([1., 1., 1.])
 
>>>#可以直接使用训练集对测试集数据进行转换
>>> scaler.transform([[-1.,  1., 0.]])                
array([[-2.44...,  1.22..., -0.26...]])
```



![v2-df795e154826d24badad42c56efadfeb_1440w](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/v2-df795e154826d24badad42c56efadfeb_1440w.jpg)

### 方法二：使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化。

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.scale(X)
 
>>> X_scaled                                          
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])
 
>>>#处理后数据的均值和方差
>>> X_scaled.mean(axis=0)
array([ 0.,  0.,  0.])
 
>>> X_scaled.std(axis=0)
array([ 1.,  1.,  1.])
```





## 归一化(Normalization，范围)

###  什么是归一化

把数据变成(0,1)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。同时把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。

### 归一化公式

通常有两种算法：

![image-20210402112528587](https://piggo-picture.oss-cn-hangzhou.aliyuncs.com/image/image-20210402112528587.png)

非线性归一化: 经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。

使用这种方法的目的包括：

1、对于方差非常小的属性可以增强其稳定性。

2、维持稀疏矩阵中为0的条目。

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5       ,  0.        ,  1.        ],
       [ 1.        ,  0.5       ,  0.33333333],
       [ 0.        ,  1.        ,  0.        ]])
 
>>> #将相同的缩放应用到测试集数据中
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])  #发现测试数据是可以超出范围1的！！！
 
 
>>> #缩放因子等属性
>>> min_max_scaler.scale_                             
array([ 0.5       ,  0.5       ,  0.33...])
 
>>> min_max_scaler.min_                               
array([ 0.        ,  0.5       ,  0.33...])
```

1. 离差标准化变量后，得到的标准差小于使用中心标准化方法。这说明在使用离差标准化后，数据的数值更加接近平均值。
2. 但是如果特征列中含有异常值（outlier）, 离差标准化只能将所有特征统一比例，并不能很好地解决异常值问题。中心标准化在异常值方面则有更好的表现，因此它比离差标准化应用更广
3. 注：如果算法不是基于距离计算，特征缩放则不重要，比如朴素贝叶斯和线性判别分析，以及树模型（梯度提升、随机森林等）。

[归一化与标准化](https://www.zhongxiaoping.cn/2019/01/15/%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/)

## 什么时候用归一化？什么时候用标准化？

（1）如果对输出结果范围有要求，用归一化。
（2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
（3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。

### 什么时候不需要转换呢？

概率模型不需要转换，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。像svm、线性回归之类的最优化问题就需要转换，决策树属于前者。

# 正则化(Regularization，1范数)

归一化和标准化是针对特征进行处理的， 而正则化是针对样本进行处理的

正则化的过程是将每个**样本**缩放到单位范数（注意，是每个**样本**的范数为1）是针对样本处理的

## 什么时候需要正则化

当需要使用二范数或者其他点积运算衡量样本间距离时，这个过程很有用

## 正则化公式

实际就是对样本进行缩放，使得其二范数为1，每个样本的缩放系数计算公式为：

```python
pow(1/sum(pow(xi,2)), 0.5)
```

## 方法

### 方法一preprocessing.normalize()函数

```python
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')
 
>>> X_normalized                                      
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])
```



### 方法二preprocessing.Normalizer()类

实现对训练集和测试集的拟合和转换：

```python
>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
>>> normalizer
Normalizer(copy=True, norm='l2')
 
>>>
>>> normalizer.transform(X)                            
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])
 
>>> normalizer.transform([[-1.,  1., 0.]])             
array([[-0.70...,  0.70...,  0.  ...]])
```



